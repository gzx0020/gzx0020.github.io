<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Few-Shot相关论文阅读笔记</title>
    <link href="/2025/06/26/Few-Shot%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2025/06/26/Few-Shot%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1>Few-Shot</h1><h2 id="Phasic-Content-Fusing-Diffusion-Model-with-Directional-Distribution-Consistency-for-Few-Shot-Model-Adaption">Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</h2><h3 id="Abstract">Abstract</h3><ol><li>当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节</li><li>引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）</li><li>跨领域情景的结构一致性</li></ol><h3 id="Challenges">Challenges</h3><ol><li>overfit</li><li>细节学习阶段（t较小的时候）风格迁移失败</li><li>现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转</li></ol><h3 id="Method">Method</h3><h4 id="Training-with-Phasic-Content-Fusion">Training with Phasic Content Fusion</h4><p>在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合$E(x^{A})$和噪声$z\sim\mathcal{N}(0,I)$，<br>$$\hat{E}(x^A)=m(t)E(x^A)+(1-m(t))z$$<br>然后使用多个卷积块将 $\hat{E}(x^A)$ 与 $E(x_{t}^{A})$ 融合，得到融合后的特征 $E(x^A,x_t^A)$ ，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的 $x_{t-1}^A$</p><h4 id="方向分布一致性损失函数-directional-distribution-consistency-loss-DDC">方向分布一致性损失函数 directional distribution consistency loss (DDC)</h4><p>最终的损失函数由以下三个损失函数构成：</p><ol><li>Directional distribution consistency loss<br>$$\mathcal{L}<em>{DDC}=|E(x^A)+w,E(x_0^{A\to B})|^2$$<br>其中w为方向向量，给定源分布 $A={x</em>{1}^{A},\cdots x_{m}^{A}}$ 和目标分布 $B={x_{1}^{B},\cdots x_{m}^{B}}$ ,特征空间中从源域中心到目标域中心的跨域方向向量w,<br>$$w=\frac{1}{m}\sum_{i=1}^mE(x_i^B)-\frac{1}{n}\sum_{i=1}^nE(x_i^A)$$</li><li>Style loss<br>$$\mathcal{L}<em>{style}=\frac{1}{m}\sum</em>{i=1}^{m}\sum_{l}w_{l}|G^{l}(x_{0}^{A\to B})-G^{l}(x_{i}^{B})|^{2}$$<br>用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵</li><li>Diffusion Loss<br>$$\mathcal{L}<em>{dif}=||\epsilon</em>\theta(x_t^B,t)-\epsilon||^2$$</li></ol><p>最终的损失函数为：</p><p>$$\mathcal{L}=m(t)(1-w(t))(\lambda_{DDC}\mathcal{L}<em>{DDC}(x^{A},x</em>{0}^{A\to B})+\lambda_{style}\mathcal{L}<em>{style}(x</em>{0}^{A\to B},x^{B}))+w(t)\mathcal{L}_{dif}(x^{B})$$</p><h4 id="迭代跨域结构引导-Iterative-Cross-domain-Structure-Guidance-ICSG">迭代跨域结构引导 Iterative Cross-domain Structure Guidance(ICSG)</h4><p>需要进一步理解</p><h3 id="实验及评估过程">实验及评估过程</h3><h3 id="相关概念">相关概念</h3><h4 id="图像翻译-Image-to-Image-Translation">图像翻译 Image-to-Image Translation</h4><p>将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。</p><h4 id="Gram矩阵">Gram矩阵</h4><h5 id="原理">原理</h5><p>n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，很明显，这是一个对称矩阵。<br>输入图像的feature map为[ ch, h, w]。我们经过flatten（即是将h* w 进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w, ch]的矩阵。再对两个作内积得到Gram矩阵。</p><h5 id="应用">应用</h5><p>Gram matrix的应用-风格迁移：</p><ol><li><p>准备基准图像和风格图像</p></li><li><p>使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature map）</p></li><li><p>分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像</p></li></ol><p>一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。</p><h4 id="消融实验-Ablation-Study">消融实验 Ablation Study</h4><p>类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。</p>]]></content>
    
    
    <categories>
      
      <category>科研实习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>week2</title>
    <link href="/2025/06/25/week2/"/>
    <url>/2025/06/25/week2/</url>
    
    <content type="html"><![CDATA[<h1>diffusion model (李宏毅)笔记</h1><h2 id="概念部分">概念部分</h2><h3 id="一般图像生成模型基本框架">一般图像生成模型基本框架</h3><p>text encoder → generation model → decoder</p><h3 id="FID-Frechet-Inception-Distance">FID(Frechet Inception Distance)</h3><p>FID是一种用于评估生成图像质量的度量标准</p><ol><li><p>特征提取  使用预训练的 Inception V3 模型（在 ImageNet 数据集上训练的图像分类网络）作为特征提取器。输入图像（通常调整为 299×299 的分辨率）会通过 Inception V3 前向传播，提取池化层（即 pool3 层）的输出特征。这个特征是一个 2048 维的向量。</p></li><li><p>特征分布假设  FID 假设提取的特征向量服从多变量正态分布。对于真实图像集合X和生成图像集合G，分别计算特征的均值向量和协方差矩阵：<br>真实图像特征均值 $\mu_{r}$   协方差 $\Sigma_{r}$<br>生成图像特征均值 $\mu_{g}$   协方差 $\Sigma_{g}$</p></li><li><p>Fréchet 距离计算<br>Fréchet 距离用来衡量两个正态分布之间的差异$$\mathrm{FID}=|\mu_r-\mu_g|_2^2+\mathrm{Tr}(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{1/2})$$<br>第一项衡量两个分布均值的欧几里得距离，表示分布中心的偏移，第二项衡量协方差矩阵的差异，反映分布形状和分散度的不同</p></li></ol><h2 id="原理部分">原理部分</h2><p><a href="https://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4">https://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4</a></p><ol><li><p>最大似然估计 <a href="https://zhuanlan.zhihu.com/p/55791843">https://zhuanlan.zhihu.com/p/55791843</a><br><img src="image%5C1747666364704.png" alt="1747666364704"></p></li><li><p>扩散模型与能量模型，Score-Matching和SDE，ODE的关系 <a href="https://zhuanlan.zhihu.com/p/576779879">https://zhuanlan.zhihu.com/p/576779879</a></p></li></ol><h3 id="疑问">疑问</h3><ol><li>李宏毅认为噪声实际上不是一步一步加进$x_{0}$的,而是一步实现的<br><img src="image%5CSnipaste_2025-05-21_18-49-49.png" alt=""><br>但通过对一致性模型的学习，我了解到diffusion model的前向过程和逆向过程实际上都能表示为SDE过程，需要进行多次迭代，而consistency model就是为了解决这个问题，将SDE的随机项消除，转变为ODE过程，从而实现减少迭代次数，这是否与上图观点相悖？</li></ol>]]></content>
    
    
    <categories>
      
      <category>科研实习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>test</title>
    <link href="/2025/05/20/test/"/>
    <url>/2025/05/20/test/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>week1</title>
    <link href="/2025/05/20/week1/"/>
    <url>/2025/05/20/week1/</url>
    
    <content type="html"><![CDATA[<h1>一致性模型（Consistency Models，CM）</h1><p><a href="https://zhuanlan.zhihu.com/p/623402026">https://zhuanlan.zhihu.com/p/623402026</a><br>一致性模型（Consistency Models，CM）主要解决扩散生成模型迭代采样过程缓慢的问题，支持一步采样快速生成和多步采样高精度生成，CM 的本质就是将任何时间步的点映射到轨迹的起点。CM 的一个关键的性质是 self-consistency 性：相同轨迹上的点映射到相同的初始点。</p><h2 id="SDE与ODE">SDE与ODE</h2><p>前向过程满足的SDE：<br>$$\mathrm{d}\mathbf{x}=\mathbf{f}(\mathbf{x},t)\mathrm{d}t+g(t)\mathrm{d}\mathbf{w}(t) $$<br>f:漂移因子 g:扩散因子 w:维纳过程(标准布朗运动)  score:$$\nabla_x\log p(x)$$ 即概率密度对数的梯度</p><p>朗之万动力学<br>边缘概率密度<br>score matching</p><p>逆向过程的SDE为：<br>$$\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g^2(t)\nabla_\mathbf{x}\log p_t(\mathbf{x})]\mathrm{d}t+g(t)\mathrm{d}\bar \ {\mathbf{w}}(t)$$</p><p>ODE：SDE去掉维纳过程，变成一个常微分方程<br>$$\mathrm{d}\mathbf{x}_t=<br>\begin{bmatrix}<br>f(\mathbf{x}_t,t)-\frac{1}{2}g^2(t)\nabla\log p_t(\mathbf{x}_t)<br>\end{bmatrix}\mathrm{d}t$$</p><h2 id="如何用神经网络训练一致性模型">如何用神经网络训练一致性模型</h2><p>一致性函数<br>$$f(\mathbf{x}<em>t,t)=<br>\begin{cases}<br>\mathbf{x}</em>\varepsilon, &amp; t=\varepsilon \<br>f(\mathbf{x}_{t^{\prime}},t^{\prime}), &amp; t\in(\varepsilon,T],\forall t^{\prime}\in[\varepsilon,T] &amp;<br>\end{cases}$$</p><p>一致性模型：即用神经网络模拟一致性函数的特性<br>给定任意神经网络F,<br>$$f_\theta(\mathbf{x}<em>t,t)=C</em>{\mathrm{skip}}(t)\mathbf{x}<em>t+C</em>{\mathrm{out}}(t)F_\theta(\mathbf{x}_t,t)$$   随t变化时C的变化</p><p>EDM–$C_{in}$</p><p>损失函数——相邻两个时间输出值差距最小化$$\mathcal{L}^N(\theta)=\mathbb{E}[|f_\theta(\mathbf{x}<em>{t</em>{n+1}},t_{n+1})-f_\theta(\hat{\mathbf{x}}<em>{t_n},t_n)|<em>2^2]$$  再经过EMA,最终$$\mathcal{L}^N(\theta,\theta^-)=\mathbb{E}[|f</em>\theta(\mathbf{x}</em>{t_{n+1}},t_{n+1})-f_{\theta^-}(\hat{\mathbf{x}}_{t_n},t_n)|_2^2]$$</p><h3 id="一致性蒸馏（简称CD，Consistency-Distillation）——从已经学好的score-function蒸馏">一致性蒸馏（简称CD，Consistency Distillation）——从已经学好的score function蒸馏</h3><p><img src="image%5C1747148746366.png" alt=""></p><p>已经有了score function $\mathbf{s}_{\phi}(\mathbf{x}(t),t)$</p><h3 id="一致性训练-简称CT，Consistency-Training-——从数据中直接学">一致性训练(简称CT，Consistency Training)——从数据中直接学</h3><p><img src="image%5CSnipaste_2025-05-15_01-19-37.png" alt=""></p><p>用$\nabla\log p_t(\mathbf{x}_t)=-\mathbb{E}\left[\frac{\mathbf{x}_t-\mathbf{x}}{t^2}|\mathbf{x}_t\right]$来代替一致性蒸馏中的已有的sore fuction</p><h2 id="如何通过一致性模型采样获得图像">如何通过一致性模型采样获得图像</h2><h3 id="一步采样">一步采样</h3><p>给定一个$x_t$，带入一致性模型</p><h3 id="多步采样">多步采样</h3><p>可提升图像质量<br><img src="" alt=""></p><h1>SR3</h1><p>SR3 is an approach to image super resolution via iterative refinement<br>通过迭代优化实现生成图像超分辨率</p><h2 id="key-words">key words</h2><ol><li>iterative refinement</li><li>both faces and natural images</li><li>bicubic interpolation</li><li>flexibility inchoosing number of diffusion steps, and the noise schedule during inference</li><li>FID</li><li>rather than estimating the posterior mean, SR3 generates samples from the target posterior.</li><li>constant number of refinement steps (often no more than 100).</li><li>onot requireanyauxiliaryobjective function inorder toensureconsistencywith the low resolutioninputs</li><li>our diffusion models do not provide a knob to control sample quality vs. sample diversity（如何平衡样本质量与样本多样性吗？）, and finding ways to do so isinteresting<br>avenue for future research.</li></ol><h2 id="涉及知识点">涉及知识点</h2><ol><li>score matching</li><li>Langevin dynamics</li><li>PSNR and SSIM</li><li>residual blocks</li><li>级联结构</li><li>Normalizing flows</li><li>anti-aliasing</li><li>ImageNet</li><li>Dropout</li></ol><h2 id="总结">总结</h2><ol><li>将LR作为条件输入</li><li>不在取离散的t，而是将同样范围内连续t的采样值（即noise）输入<br>或许可以减小推理步数，加快速度？</li><li>级联 分阶段生成：<br>第一阶段：使用无条件生成模型（如DDPM）生成低分辨率图像（如64×64）。<br>第二阶段：将低分辨率图像输入第一个SR3模型，进行4倍上采样（64→256）。<br>第三阶段：将256×256图像输入第二个SR3模型，再次4倍上采样至1024×1024。</li></ol><h2 id="问题">问题</h2><ol><li>下采样操作，将 HR 图像的尺⼨减半，⽣成对应的 LR 图像<br>下采样方式如何选择（是否采用SR3论文中提到的双三次插值？），以及为什么规定LR为HR尺寸减半后的结果</li><li>SR3采用的是迭代优化实现图像超分辨率重建的方法，是否面临计算和时间成本高的问题，如何解决是否可以参考连续一致性模型的做法</li><li>连续一致性模型有单步采样和多部采样两种方式，多部采样可以理解为牺牲速度换取高质量？是否可以再次基础上实现超分辨率重建？</li><li>尝试<a href="https://github.com/openai/consistency_models">https://github.com/openai/consistency_models</a><br>和<a href="https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement">https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement</a> 时遇到困难</li><li>对数学公式的推导要掌握到什么程度？</li></ol>]]></content>
    
    
    <categories>
      
      <category>科研实习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
