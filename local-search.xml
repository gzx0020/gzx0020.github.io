<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LoRA</title>
    <link href="/2025/06/26/LoRA/"/>
    <url>/2025/06/26/LoRA/</url>
    
    <content type="html"><![CDATA[<h1 id="一.lora原理">一.LoRA原理</h1><p>参考：<a href="https://zhuanlan.zhihu.com/p/702629428"class="uri">https://zhuanlan.zhihu.com/p/702629428</a> 原论文：<ahref="https://arxiv.org/pdf/2106.09685"class="uri">https://arxiv.org/pdf/2106.09685</a> LoRA(Low-RankAdaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。</p><p>LoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。</p><h2 id="实现流程">1.1实现流程</h2><p><img src="v2-10ce9e224defb3732e09a257911821aa_1440w.png" /></p><ol type="1"><li>在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；</li><li>用随机高斯分布初始化A，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵B；</li><li>训练完成后，将 B 矩阵与 A矩阵相乘后合并预训练模型参数作为微调后的模型参数。</li></ol><p>具体来讲，预训练权重矩阵 <spanclass="math inline"><strong>W</strong><sub>0</sub> ∈ ℝ<sup><em>d</em> × <em>d</em></sup></span>， 将增量参数矩阵 <spanclass="math inline"><em>Δ</em><strong>W</strong></span>，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式 <spanclass="math display"><strong>W</strong><sub>0</sub> + <em>Δ</em><strong>W</strong> = <strong>W</strong><sub>0</sub> + <strong>B</strong><strong>A</strong></span>其中 <spanclass="math inline"><strong>B</strong> ∈ ℝ<sup><em>d</em> × <em>r</em></sup></span>，<spanclass="math inline"><strong>A</strong> ∈ ℝ<sup><em>r</em> × <em>d</em></sup></span>，秩<span class="math inline">r</span>远小于<spanclass="math inline">d</span> 给定输入<spanclass="math inline">.<strong>x</strong> ∈ ℝ<sup><em>d</em></sup></span>,添加LoRA后的输出<spanclass="math inline">.<strong>h</strong> ∈ ℝ<sup><em>d</em></sup></span><spanclass="math display"><strong>h</strong> = (<strong>W</strong><sub>0</sub> + <em>Δ</em><strong>W</strong>)<strong>x</strong> = <strong>W</strong><sub>0</sub><strong>x</strong> + <strong>B</strong><strong>A</strong><strong>x</strong></span><spanclass="math display"><em>Δ</em><strong>h</strong> = <strong>B</strong><strong>A</strong><strong>x</strong></span></p><h2 id="lora参数合并系数">1.2LoRA参数合并系数</h2><p>实际实现时以以下形式合并，其中<spanclass="math inline"><em>α</em></span>为超参数 <spanclass="math display">$$\mathbf{h}=(\mathbf{W}_{0}+\frac{\alpha}{r}\Delta\mathbf{W})\mathbf{x}$$</span>系数<spanclass="math inline">$\frac{\alpha}{r}$</span>越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合系数<spanclass="math inline">$\frac{\alpha}{r}$</span>越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）一般来说，在给定任务上LoRA微调，让<spanclass="math inline"><em>α</em></span>为<spanclass="math inline"><strong>r</strong></span>的2倍数。</p><h2 id="lora的秩mathbfr如何选择">1.3 LoRA的秩<spanclass="math inline"><strong>r</strong></span>如何选择</h2><p>目标：找到一个秩<spanclass="math inline"><strong>r</strong></span>，使<spanclass="math inline">BA</span>无限接近<spanclass="math inline"><em>Δ</em><strong>W</strong></span>的表达能力。秩<spanclass="math inline"><strong>r</strong></span>越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。</p>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -RadioDiff微调</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>LoRA</title>
    <link href="/2025/06/26/LoRA/LoRA/"/>
    <url>/2025/06/26/LoRA/LoRA/</url>
    
    <content type="html"><![CDATA[<h1 id="一.lora原理">一.LoRA原理</h1><p>参考：<a href="https://zhuanlan.zhihu.com/p/702629428"class="uri">https://zhuanlan.zhihu.com/p/702629428</a> 原论文：<ahref="https://arxiv.org/pdf/2106.09685"class="uri">https://arxiv.org/pdf/2106.09685</a> LoRA(Low-RankAdaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。</p><p>LoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。</p><h2 id="实现流程">1.1实现流程</h2><p><img src="v2-10ce9e224defb3732e09a257911821aa_1440w.png" /></p><ol type="1"><li>在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；</li><li>用随机高斯分布初始化A，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵B；</li><li>训练完成后，将 B 矩阵与 A矩阵相乘后合并预训练模型参数作为微调后的模型参数。</li></ol><p>具体来讲，预训练权重矩阵 <spanclass="math inline"><strong>W</strong><sub>0</sub> ∈ ℝ<sup><em>d</em> × <em>d</em></sup></span>， 将增量参数矩阵 <spanclass="math inline"><em>Δ</em><strong>W</strong></span>，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式 <spanclass="math display"><strong>W</strong><sub>0</sub> + <em>Δ</em><strong>W</strong> = <strong>W</strong><sub>0</sub> + <strong>B</strong><strong>A</strong></span>其中 <spanclass="math inline"><strong>B</strong> ∈ ℝ<sup><em>d</em> × <em>r</em></sup></span>，<spanclass="math inline"><strong>A</strong> ∈ ℝ<sup><em>r</em> × <em>d</em></sup></span>，秩<span class="math inline">r</span>远小于<spanclass="math inline">d</span> 给定输入<spanclass="math inline">.<strong>x</strong> ∈ ℝ<sup><em>d</em></sup></span>,添加LoRA后的输出<spanclass="math inline">.<strong>h</strong> ∈ ℝ<sup><em>d</em></sup></span><spanclass="math display"><strong>h</strong> = (<strong>W</strong><sub>0</sub> + <em>Δ</em><strong>W</strong>)<strong>x</strong> = <strong>W</strong><sub>0</sub><strong>x</strong> + <strong>B</strong><strong>A</strong><strong>x</strong></span><spanclass="math display"><em>Δ</em><strong>h</strong> = <strong>B</strong><strong>A</strong><strong>x</strong></span></p><h2 id="lora参数合并系数">1.2LoRA参数合并系数</h2><p>实际实现时以以下形式合并，其中<spanclass="math inline"><em>α</em></span>为超参数 <spanclass="math display">$$\mathbf{h}=(\mathbf{W}_{0}+\frac{\alpha}{r}\Delta\mathbf{W})\mathbf{x}$$</span>系数<spanclass="math inline">$\frac{\alpha}{r}$</span>越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合系数<spanclass="math inline">$\frac{\alpha}{r}$</span>越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）一般来说，在给定任务上LoRA微调，让<spanclass="math inline"><em>α</em></span>为<spanclass="math inline"><strong>r</strong></span>的2倍数。</p><h2 id="lora的秩mathbfr如何选择">1.3 LoRA的秩<spanclass="math inline"><strong>r</strong></span>如何选择</h2><p>目标：找到一个秩<spanclass="math inline"><strong>r</strong></span>，使<spanclass="math inline">BA</span>无限接近<spanclass="math inline"><em>Δ</em><strong>W</strong></span>的表达能力。秩<spanclass="math inline"><strong>r</strong></span>越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。</p>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -RadioDiff微调</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Specialist Diffusion</title>
    <link href="/2025/06/26/Specialist-Diffusion/"/>
    <url>/2025/06/26/Specialist-Diffusion/</url>
    
    <content type="html"><![CDATA[<h1id="specialist-diffusion-plug-and-play-sample-efficient-fine-tuning-of-text-to-image-diffusion-models-to-learn-any-unseen-style">SpecialistDiffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-ImageDiffusion Models to Learn Any Unseen Style</h1><p><a href="https://arxiv.org/pdf/2211.12572"class="uri">https://arxiv.org/pdf/2211.12572</a></p><h2 id="主要内容">主要内容</h2><p>这篇论文提出SpecialistDiffusion：相当于一个即插即用的微调工具包，包括文本到图像的定制数据增强，contentloss to facilitate content-style disentanglement，sparsely updatingdiffusion timesteps。主要适用于以少量已知风格的图片训练模型，使其能够通过特定的文本提示生成相应风格的图片。</p><h3 id="data-augmentations-for-text2image-diffusion数据增强">DataAugmentations for Text2Image Diffusion（数据增强）</h3><h4 id="image-augmentation">Image Augmentation</h4><h4 id="text-prompt-augmentation">Text Prompt Augmentation</h4><h5 id="caption-retrieval-augmentation-标题搜索增强">Caption RetrievalAugmentation 标题搜索增强</h5><h5 id="synonym-augmentation-同义词增强">Synonym Augmentation同义词增强</h5><h5 id="doubled-augmentation-双重增强">Doubled Augmentation双重增强</h5><h3 id="content-loss">Content Loss</h3><h3 id="sparse-updating">Sparse Updating</h3><h2 id="相关概念">相关概念</h2><h3 id="增强泄露问题-augmentation-leakage">增强泄露问题 augmentationleakage</h3><p>生成模型在训练过程中，会记住训练样本及其经过增强后的版本，这样在推理（生成新内容）阶段，就容易生成与训练时相似的图像。举个文中例子，很多旋转后的图像理论上算自然照片，但在真实自然图像集合里，它们出现的概率其实更低。要是训练时过度用旋转增强，模型就可能“bias（偏向）” 生成更多带旋转的物体，可这些并非实际想要的（“un - tended”，即不符合自然场景常见分布 ），相当于增强操作的影响 “泄漏”到生成结果里，让生成内容偏离真实自然数据的合理分布，这就是 “augmentationleakage” 。简单说，就是数据增强的不当使用，让模型学到了增强带来的“虚假模式”，而非真实场景的合理特征，影响生成效果。</p><h3 id="正则化-regularization">正则化 Regularization</h3><p><strong>正则化是用来防止模型过拟合而采取的手段</strong>，对代价函数增加一个限制条件，限制其较高次的参数大小不能过大参考：<ahref="https://blog.csdn.net/weixin_41960890/article/details/104891561"class="uri">https://blog.csdn.net/weixin_41960890/article/details/104891561</a></p>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -RadioDiff微调 -Few-Shot相关论文阅读笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</title>
    <link href="/2025/06/26/Phasic%20Content%20Fusing%20Diffusion%20Model%20with%20Directional%20Distribution%20Consistency%20for%20Few-Shot%20Model%20Adaption/"/>
    <url>/2025/06/26/Phasic%20Content%20Fusing%20Diffusion%20Model%20with%20Directional%20Distribution%20Consistency%20for%20Few-Shot%20Model%20Adaption/</url>
    
    <content type="html"><![CDATA[<h1 id="few-shot">Few-Shot</h1><h2id="phasic-content-fusing-diffusion-model-with-directional-distribution-consistency-for-few-shot-model-adaption">PhasicContent Fusing Diffusion Model with Directional Distribution Consistencyfor Few-Shot Model Adaption</h2><h3 id="abstract">Abstract</h3><ol type="1"><li>当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节</li><li>引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）</li><li>跨领域情景的结构一致性</li></ol><h3 id="challenges">Challenges</h3><ol type="1"><li>overfit</li><li>细节学习阶段（t较小的时候）风格迁移失败</li><li>现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转</li></ol><h3 id="method">Method</h3><h4 id="training-with-phasic-content-fusion">Training with PhasicContent Fusion</h4><p>在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合<spanclass="math inline"><em>E</em>(<em>x</em><sup><em>A</em></sup>)</span>和噪声<spanclass="math inline"><em>z</em> ∼ 𝒩(0, <em>I</em>)</span>， <spanclass="math display"><em>Ê</em>(<em>x</em><sup><em>A</em></sup>) = <em>m</em>(<em>t</em>)<em>E</em>(<em>x</em><sup><em>A</em></sup>) + (1 − <em>m</em>(<em>t</em>))<em>z</em></span>然后使用多个卷积块将 <spanclass="math inline"><em>Ê</em>(<em>x</em><sup><em>A</em></sup>)</span>与 <spanclass="math inline"><em>E</em>(<em>x</em><sub><em>t</em></sub><sup><em>A</em></sup>)</span>融合，得到融合后的特征 <spanclass="math inline"><em>E</em>(<em>x</em><sup><em>A</em></sup>, <em>x</em><sub><em>t</em></sub><sup><em>A</em></sup>)</span>，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的<spanclass="math inline"><em>x</em><sub><em>t</em> − 1</sub><sup><em>A</em></sup></span>#### 方向分布一致性损失函数 directional distribution consistency loss(DDC) 最终的损失函数由以下三个损失函数构成： 1. Directional distributionconsistency loss <spanclass="math display">ℒ<sub><em>D</em><em>D</em><em>C</em></sub> = ∥<em>E</em>(<em>x</em><sup><em>A</em></sup>) + <em>w</em>, <em>E</em>(<em>x</em><sub>0</sub><sup><em>A</em> → <em>B</em></sup>)∥<sup>2</sup></span>其中w为方向向量，给定源分布 <spanclass="math inline"><em>A</em> = {<em>x</em><sub>1</sub><sup><em>A</em></sup>, ⋯<em>x</em><sub><em>m</em></sub><sup><em>A</em></sup>}</span>和目标分布 <spanclass="math inline"><em>B</em> = {<em>x</em><sub>1</sub><sup><em>B</em></sup>, ⋯<em>x</em><sub><em>m</em></sub><sup><em>B</em></sup>}</span>,特征空间中从源域中心到目标域中心的跨域方向向量w, <spanclass="math display">$$w=\frac{1}{m}\sum_{i=1}^mE(x_i^B)-\frac{1}{n}\sum_{i=1}^nE(x_i^A)$$</span>2. Style loss <spanclass="math display">$$\mathcal{L}_{style}=\frac{1}{m}\sum_{i=1}^{m}\sum_{l}w_{l}\|G^{l}(x_{0}^{A\toB})-G^{l}(x_{i}^{B})\|^{2}$$</span>用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵 3. Diffusion Loss<spanclass="math display">ℒ<sub><em>d</em><em>i</em><em>f</em></sub> = ||<em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub><sup><em>B</em></sup>, <em>t</em>) − <em>ϵ</em>||<sup>2</sup></span></p><p>最终的损失函数为：</p><p><spanclass="math display">ℒ = <em>m</em>(<em>t</em>)(1 − <em>w</em>(<em>t</em>))(<em>λ</em><sub><em>D</em><em>D</em><em>C</em></sub>ℒ<sub><em>D</em><em>D</em><em>C</em></sub>(<em>x</em><sup><em>A</em></sup>, <em>x</em><sub>0</sub><sup><em>A</em> → <em>B</em></sup>) + <em>λ</em><sub><em>s</em><em>t</em><em>y</em><em>l</em><em>e</em></sub>ℒ<sub><em>s</em><em>t</em><em>y</em><em>l</em><em>e</em></sub>(<em>x</em><sub>0</sub><sup><em>A</em> → <em>B</em></sup>, <em>x</em><sup><em>B</em></sup>)) + <em>w</em>(<em>t</em>)ℒ<sub><em>d</em><em>i</em><em>f</em></sub>(<em>x</em><sup><em>B</em></sup>)</span></p><h4id="迭代跨域结构引导-iterative-cross-domain-structure-guidanceicsg">迭代跨域结构引导Iterative Cross-domain Structure Guidance(ICSG)</h4><p>需要进一步理解</p><h3 id="实验及评估过程">实验及评估过程</h3><h3 id="相关概念">相关概念</h3><h4 id="图像翻译-image-to-image-translation">图像翻译 Image-to-ImageTranslation</h4><p>将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。</p><h4 id="gram矩阵">Gram矩阵</h4><h5 id="原理">原理</h5><p>n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Grammatrix)，很明显，这是一个对称矩阵。 输入图像的feature map为[ ch, h,w]。我们经过flatten（即是将h* w进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w,ch]的矩阵。再对两个作内积得到Gram矩阵。</p><h5 id="应用">应用</h5><p>Gram matrix的应用-风格迁移： 1. 准备基准图像和风格图像</p><ol start="2" type="1"><li><p>使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图featuremap）</p></li><li><p>分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像</p></li></ol><p>一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。</p><h4 id="消融实验-ablation-study">消融实验 Ablation Study</h4><p>类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。</p>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -RadioDiff微调 -Few-Shot相关论文阅读笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>week2</title>
    <link href="/2025/06/25/week2/"/>
    <url>/2025/06/25/week2/</url>
    
    <content type="html"><![CDATA[<h1 id="diffusion-model-李宏毅笔记">diffusion model (李宏毅)笔记</h1><h2 id="概念部分">概念部分</h2><h3 id="一般图像生成模型基本框架">一般图像生成模型基本框架</h3><p>text encoder → generation model → decoder</p><h3 id="fidfrechet-inception-distance">FID(Frechet InceptionDistance)</h3><p>FID是一种用于评估生成图像质量的度量标准</p><ol type="1"><li><p>特征提取 使用预训练的 Inception V3 模型（在 ImageNet数据集上训练的图像分类网络）作为特征提取器。输入图像（通常调整为 299×299的分辨率）会通过 Inception V3 前向传播，提取池化层（即 pool3层）的输出特征。这个特征是一个 2048 维的向量。</p></li><li><p>特征分布假设 FID假设提取的特征向量服从多变量正态分布。对于真实图像集合X和生成图像集合G，分别计算特征的均值向量和协方差矩阵：真实图像特征均值 <spanclass="math inline"><em>μ</em><sub><em>r</em></sub></span> 协方差 <spanclass="math inline"><em>Σ</em><sub><em>r</em></sub></span>生成图像特征均值 <spanclass="math inline"><em>μ</em><sub><em>g</em></sub></span> 协方差 <spanclass="math inline"><em>Σ</em><sub><em>g</em></sub></span></p></li><li><p>Fréchet 距离计算 Fréchet 距离用来衡量两个正态分布之间的差异<spanclass="math display">FID = ∥<em>μ</em><sub><em>r</em></sub> − <em>μ</em><sub><em>g</em></sub>∥<sub>2</sub><sup>2</sup> + Tr(<em>Σ</em><sub><em>r</em></sub> + <em>Σ</em><sub><em>g</em></sub> − 2(<em>Σ</em><sub><em>r</em></sub><em>Σ</em><sub><em>g</em></sub>)<sup>1/2</sup>)</span>第一项衡量两个分布均值的欧几里得距离，表示分布中心的偏移，第二项衡量协方差矩阵的差异，反映分布形状和分散度的不同</p></li></ol><h2 id="原理部分">原理部分</h2><p><ahref="https://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4"class="uri">https://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4</a></p><ol type="1"><li><p>最大似然估计 <a href="https://zhuanlan.zhihu.com/p/55791843"class="uri">https://zhuanlan.zhihu.com/p/55791843</a> <imgsrc="image\1747666364704.png" alt="1747666364704" /></p></li><li><p>扩散模型与能量模型，Score-Matching和SDE，ODE的关系 <ahref="https://zhuanlan.zhihu.com/p/576779879"class="uri">https://zhuanlan.zhihu.com/p/576779879</a></p></li></ol><h3 id="疑问">疑问</h3><ol type="1"><li>李宏毅认为噪声实际上不是一步一步加进<spanclass="math inline"><em>x</em><sub>0</sub></span>的,而是一步实现的 <imgsrc="image\Snipaste_2025-05-21_18-49-49.png" />但通过对一致性模型的学习，我了解到diffusionmodel的前向过程和逆向过程实际上都能表示为SDE过程，需要进行多次迭代，而consistencymodel就是为了解决这个问题，将SDE的随机项消除，转变为ODE过程，从而实现减少迭代次数，这是否与上图观点相悖？</li></ol>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -周记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>week1</title>
    <link href="/2025/05/20/week1/"/>
    <url>/2025/05/20/week1/</url>
    
    <content type="html"><![CDATA[<h1 id="一致性模型consistency-modelscm">一致性模型（ConsistencyModels，CM）</h1><p><a href="https://zhuanlan.zhihu.com/p/623402026"class="uri">https://zhuanlan.zhihu.com/p/623402026</a>一致性模型（ConsistencyModels，CM）主要解决扩散生成模型迭代采样过程缓慢的问题，支持一步采样快速生成和多步采样高精度生成，CM的本质就是将任何时间步的点映射到轨迹的起点。CM 的一个关键的性质是self-consistency 性：相同轨迹上的点映射到相同的初始点。</p><h2 id="sde与ode">SDE与ODE</h2><p>前向过程满足的SDE： <spanclass="math display">d<strong>x</strong> = <strong>f</strong>(<strong>x</strong>, <em>t</em>)d<em>t</em> + <em>g</em>(<em>t</em>)d<strong>w</strong>(<em>t</em>)</span>f:漂移因子 g:扩散因子 w:维纳过程(标准布朗运动) score:<spanclass="math display">∇<sub><em>x</em></sub>log <em>p</em>(<em>x</em>)</span>即概率密度对数的梯度</p><p>朗之万动力学<br />边缘概率密度<br />score matching</p><p>逆向过程的SDE为： <spanclass="math display">$$\mathrm{d}\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g^2(t)\nabla_\mathbf{x}\logp_t(\mathbf{x})]\mathrm{d}t+g(t)\mathrm{d}\bar \\{\mathbf{w}}(t)$$</span></p><p>ODE：SDE去掉维纳过程，变成一个常微分方程<br /><span class="math display">$$\mathrm{d}\mathbf{x}_t=\begin{bmatrix}f(\mathbf{x}_t,t)-\frac{1}{2}g^2(t)\nabla\log p_t(\mathbf{x}_t)\end{bmatrix}\mathrm{d}t$$</span></p><h2 id="如何用神经网络训练一致性模型">如何用神经网络训练一致性模型</h2><p>一致性函数 <span class="math display">$$f(\mathbf{x}_t,t)=\begin{cases}\mathbf{x}_\varepsilon, &amp; t=\varepsilon \\f(\mathbf{x}_{t^{\prime}},t^{\prime}), &amp; t\in(\varepsilon,T],\forallt^{\prime}\in[\varepsilon,T] &amp;\end{cases}$$</span></p><p>一致性模型：即用神经网络模拟一致性函数的特性<br />给定任意神经网络F, <spanclass="math display"><em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>) = <em>C</em><sub>skip</sub>(<em>t</em>)<strong>x</strong><sub><em>t</em></sub> + <em>C</em><sub>out</sub>(<em>t</em>)<em>F</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>)</span>随t变化时C的变化</p><p>EDM–<spanclass="math inline"><em>C</em><sub><em>i</em><em>n</em></sub></span></p><p>损失函数——相邻两个时间输出值差距最小化<spanclass="math display">$$\mathcal{L}^N(\theta)=\mathbb{E}[\|f_\theta(\mathbf{x}_{t_{n+1}},t_{n+1})-f_\theta(\hat{\mathbf{x}}_{t_n},t_n)\|_2^2]$$</span>再经过EMA,最终<spanclass="math display">$$\mathcal{L}^N(\theta,\theta^-)=\mathbb{E}[\|f_\theta(\mathbf{x}_{t_{n+1}},t_{n+1})-f_{\theta^-}(\hat{\mathbf{x}}_{t_n},t_n)\|_2^2]$$</span></p><h3id="一致性蒸馏简称cdconsistency-distillation从已经学好的score-function蒸馏">一致性蒸馏（简称CD，ConsistencyDistillation）——从已经学好的score function蒸馏</h3><p><img src="1747148746366.png" /></p><p>已经有了score function <spanclass="math inline"><strong>s</strong><sub><em>ϕ</em></sub>(<strong>x</strong>(<em>t</em>), <em>t</em>)</span>### 一致性训练(简称CT，Consistency Training)——从数据中直接学 <imgsrc="Snipaste_2025-05-15_01-19-37.png" /></p><p>用<span class="math inline">$\nabla\logp_t(\mathbf{x}_t)=-\mathbb{E}\left[\frac{\mathbf{x}_t-\mathbf{x}}{t^2}|\mathbf{x}_t\right]$</span>来代替一致性蒸馏中的已有的sorefuction</p><h2id="如何通过一致性模型采样获得图像">如何通过一致性模型采样获得图像</h2><h3 id="一步采样">一步采样</h3><p>给定一个<spanclass="math inline"><em>x</em><sub><em>t</em></sub></span>，带入一致性模型</p><h3 id="多步采样">多步采样</h3><p>可提升图像质量</p><h1 id="sr3">SR3</h1><p>SR3 is an approach to image super resolution via iterative refinement通过迭代优化实现生成图像超分辨率</p><h2 id="key-words">key words</h2><ol type="1"><li>iterative refinement</li><li>both faces and natural images</li><li>bicubic interpolation</li><li>flexibility inchoosing number of diffusion steps, and the noiseschedule during inference</li><li>FID</li><li>rather than estimating the posterior mean, SR3 generates samplesfrom the target posterior.</li><li>constant number of refinement steps (often no more than 100).</li><li>onot requireanyauxiliaryobjective function inordertoensureconsistencywith the low resolutioninputs</li><li>our diffusion models do not provide a knob to control sample qualityvs. sample diversity（如何平衡样本质量与样本多样性吗？）, and findingways to do so isinteresting avenue for future research.</li></ol><h2 id="涉及知识点">涉及知识点</h2><ol type="1"><li>score matching</li><li>Langevin dynamics</li><li>PSNR and SSIM</li><li>residual blocks</li><li>级联结构</li><li>Normalizing flows</li><li>anti-aliasing</li><li>ImageNet</li><li>Dropout</li></ol><h2 id="总结">总结</h2><ol type="1"><li>将LR作为条件输入</li><li>不在取离散的t，而是将同样范围内连续t的采样值（即noise）输入或许可以减小推理步数，加快速度？</li><li>级联 分阶段生成：第一阶段：使用无条件生成模型（如DDPM）生成低分辨率图像（如64×64）。第二阶段：将低分辨率图像输入第一个SR3模型，进行4倍上采样（64→256）。第三阶段：将256×256图像输入第二个SR3模型，再次4倍上采样至1024×1024。</li></ol><h2 id="问题">问题</h2><ol type="1"><li>下采样操作，将 HR 图像的尺⼨减半，⽣成对应的 LR 图像下采样方式如何选择（是否采用SR3论文中提到的双三次插值？），以及为什么规定LR为HR尺寸减半后的结果</li><li>SR3采用的是迭代优化实现图像超分辨率重建的方法，是否面临计算和时间成本高的问题，如何解决是否可以参考连续一致性模型的做法</li><li>连续一致性模型有单步采样和多部采样两种方式，多部采样可以理解为牺牲速度换取高质量？是否可以再次基础上实现超分辨率重建？</li><li>尝试<a href="https://github.com/openai/consistency_models"class="uri">https://github.com/openai/consistency_models</a> 和<ahref="https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement"class="uri">https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement</a>时遇到困难</li><li>对数学公式的推导要掌握到什么程度？</li></ol>]]></content>
    
    
    <categories>
      
      <category>-科研实习 -周记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
