[{"title":"LoRA","url":"/2025/06/26/LoRA/","content":"一.LoRA原理\n参考：https://zhuanlan.zhihu.com/p/702629428\n原论文：https://arxiv.org/pdf/2106.09685\nLoRA(Low-Rank Adaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。\nLoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。\n1.1实现流程\n\n\n在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；\n用随机高斯分布初始化 A，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵 B；\n训练完成后，将 B 矩阵与 A 矩阵相乘后合并预训练模型参数作为微调后的模型参数。\n\n具体来讲，预训练权重矩阵 $\\mathbf{W}_0\\in\\mathbb{R}^{d\\times d}$ ，\n将增量参数矩阵 $\\Delta\\mathbf{W}$ ，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式\n$$\\mathbf{W}{0}+\\Delta\\mathbf{W}=\\mathbf{W}{0}+\\mathbf{BA}$$\n其中 $\\mathbf{B}\\in\\mathbb{R}^{d\\times r}$ ，$\\mathbf{A}\\in\\mathbb{R}^{r\\times d}$ ，秩$\\mathrm{r}$远小于$\\mathrm{d}$\n给定输入$.\\mathbf{x}\\in\\mathbb{R}^d$ ,添加LoRA后的输出$.\\mathbf{h}\\in\\mathbb{R}^d$\n$$\\mathbf{h}=(\\mathbf{W}{0}+\\Delta\\mathbf{W})\\mathbf{x}=\\mathbf{W}{0}\\mathbf{x}+\\mathbf{BA}\\mathbf{x}$$\n$$\\Delta\\mathbf{h}=\\mathbf{BAx}$$\n1.2LoRA参数合并系数\n实际实现时以以下形式合并，其中$\\alpha$为超参数\n$$\\mathbf{h}=(\\mathbf{W}_{0}+\\frac{\\alpha}{r}\\Delta\\mathbf{W})\\mathbf{x}$$\n系数$\\frac{\\alpha}{r}$越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合\n系数$\\frac{\\alpha}{r}$越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）\n一般来说，在给定任务上LoRA微调，让$\\alpha$为$\\mathbf{r}$的2倍数。\n1.3 LoRA的秩$\\mathbf{r}$如何选择\n目标：找到一个秩$\\mathbf{r}$，使$\\mathrm{BA}$无限接近$\\Delta\\mathbf{W}$的表达能力。\n秩$\\mathbf{r}$越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。\n","categories":["-科研实习 -RadioDiff微调"]},{"title":"Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption","url":"/2025/06/26/Phasic%20Content%20Fusing%20Diffusion%20Model%20with%20Directional%20Distribution%20Consistency%20for%20Few-Shot%20Model%20Adaption/","content":"Few-Shot\nPhasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption\nAbstract\n\n当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节\n引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）\n跨领域情景的结构一致性\n\nChallenges\n\noverfit\n细节学习阶段（t较小的时候）风格迁移失败\n现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转\n\nMethod\nTraining with Phasic Content Fusion\n在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合$E(x^{A})$和噪声$z\\sim\\mathcal{N}(0,I)$，\n$\\hat{E}(x^A)=m(t)E(x^A)+(1-m(t))z$\n然后使用多个卷积块将 $\\hat{E}(x^A)$ 与 $E(x_{t}^{A})$ 融合，得到融合后的特征 $E(x^A,x_t^A)$ ，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的 $x_{t-1}^A$\n方向分布一致性损失函数 directional distribution consistency loss (DDC)\n最终的损失函数由以下三个损失函数构成：\n\n\nDirectional distribution consistency loss\n$\\mathcal{L}_{DDC}=|E(x^A)+w,E(x_0^{A\\to B})|^2$\n其中w为方向向量，给定源分布 $A={x_{1}^{A},\\cdots x_{m}^{A}}$ 和目标分布 $B={x_{1}^{B},\\cdots x_{m}^{B}}$ ,特征空间中从源域中心到目标域中心的跨域方向向量w,\n$w=\\frac{1}{m}\\sum_{i=1}^mE(x_i^B)-\\frac{1}{n}\\sum_{i=1}^nE(x_i^A)$\n\n\nStyle loss\n$\\mathcal{L}{style}=\\frac{1}{m}\\sum{i=1}^{m}\\sum_{l}w_{l}|G^{l}(x_{0}^{A\\to B})-G^{l}(x_{i}^{B})|^{2}$\n用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵\n\n\nDiffusion Loss\n\n\n$\\mathcal{L}{dif}=||\\epsilon\\theta(x_t^B,t)-\\epsilon||^2$\n\n\n最终的损失函数为：\n$\\mathcal{L}=m(t)(1-w(t))(\\lambda_{DDC}\\mathcal{L}{DDC}(x^{A},x{0}^{A\\to B})+\\lambda_{style}\\mathcal{L}{style}(x{0}^{A\\to B},x^{B}))+w(t)\\mathcal{L}_{dif}(x^{B})$\n迭代跨域结构引导 Iterative Cross-domain Structure Guidance(ICSG)\n需要进一步理解\n实验及评估过程\n相关概念\n图像翻译 Image-to-Image Translation\n将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。\nGram矩阵\n原理\nn维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，很明显，这是一个对称矩阵。\n输入图像的feature map为[ ch, h, w]。我们经过flatten（即是将h* w 进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w, ch]的矩阵。再对两个作内积得到Gram矩阵。\n应用\nGram matrix的应用-风格迁移：\n\n\n准备基准图像和风格图像\n\n\n使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature map）\n\n\n分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像\n\n\n一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。\n消融实验 Ablation Study\n类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。\n","categories":["论文阅读笔记"]},{"title":"Specialist Diffusion","url":"/2025/06/26/Specialist-Diffusion/","content":"Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style\nhttps://arxiv.org/pdf/2211.12572\n主要内容\n这篇论文提出Specialist Diffusion：相当于一个即插即用的微调工具包，包括文本到图像的定制数据增强，content loss to facilitate content-style disentanglement，sparsely updating diffusion time steps。主要适用于以少量已知风格的图片训练模型，使其能够通过特定的文本提示生成相应风格的图片。\nData Augmentations for Text2Image Diffusion（数据增强）\nImage Augmentation\nText Prompt Augmentation\nCaption Retrieval Augmentation 标题搜索增强\nSynonym Augmentation 同义词增强\nDoubled Augmentation 双重增强\nContent Loss\nSparse Updating\n相关概念\n增强泄露问题 augmentation leakage\n生成模型在训练过程中，会记住训练样本及其经过增强后的版本，这样在推理（生成新内容）阶段，就容易生成与训练时相似的图像。\n举个文中例子，很多旋转后的图像理论上算自然照片，但在真实自然图像集合里，它们出现的概率其实更低。要是训练时过度用旋转增强，模型就可能 “bias（偏向）” 生成更多带旋转的物体，可这些并非实际想要的（“un - tended” ，即不符合自然场景常见分布 ），相当于增强操作的影响 “泄漏” 到生成结果里，让生成内容偏离真实自然数据的合理分布，这就是 “augmentation leakage” 。简单说，就是数据增强的不当使用，让模型学到了增强带来的 “虚假模式”，而非真实场景的合理特征，影响生成效果。\n正则化 Regularization\n正则化是用来防止模型过拟合而采取的手段，对代价函数增加一个限制条件，限制其较高次的参数大小不能过大\n参考：https://blog.csdn.net/weixin_41960890/article/details/104891561\n","categories":["论文阅读笔记"]},{"title":"week1","url":"/2025/05/20/week1/","content":"一致性模型（Consistency Models，CM）\nhttps://zhuanlan.zhihu.com/p/623402026\n一致性模型（Consistency Models，CM）主要解决扩散生成模型迭代采样过程缓慢的问题，支持一步采样快速生成和多步采样高精度生成，CM 的本质就是将任何时间步的点映射到轨迹的起点。CM 的一个关键的性质是 self-consistency 性：相同轨迹上的点映射到相同的初始点。\nSDE与ODE\n前向过程满足的SDE：\n$\\mathrm{d}\\mathbf{x}=\\mathbf{f}(\\mathbf{x},t)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w}(t) $\nf:漂移因子 g:扩散因子 w:维纳过程(标准布朗运动)  score:$\\nabla_x\\log p(x)$ 即概率密度对数的梯度\n朗之万动力学\n边缘概率密度\nscore matching\n逆向过程的SDE为：\n$\\mathrm{d}\\mathbf{x}=[\\mathbf{f}(\\mathbf{x},t)-g^2(t)\\nabla_\\mathbf{x}\\log p_t(\\mathbf{x})]\\mathrm{d}t+g(t)\\mathrm{d}\\bar \\ {\\mathbf{w}}(t)$\nODE：SDE去掉维纳过程，变成一个常微分方程\n$\\mathrm{d}\\mathbf{x}_t=\n\\begin{bmatrix}\nf(\\mathbf{x}_t,t)-\\frac{1}{2}g^2(t)\\nabla\\log p_t(\\mathbf{x}_t)\n\\end{bmatrix}\\mathrm{d}t$\n如何用神经网络训练一致性模型\n一致性函数\n$f(\\mathbf{x}t,t)=\n\\begin{cases}\n\\mathbf{x}\\varepsilon, &amp; t=\\varepsilon \\\nf(\\mathbf{x}_{t^{\\prime}},t^{\\prime}), &amp; t\\in(\\varepsilon,T],\\forall t^{\\prime}\\in[\\varepsilon,T] &amp;\n\\end{cases}$\n一致性模型：即用神经网络模拟一致性函数的特性\n给定任意神经网络F,\n$f_\\theta(\\mathbf{x}t,t)=C{\\mathrm{skip}}(t)\\mathbf{x}t+C{\\mathrm{out}}(t)F_\\theta(\\mathbf{x}_t,t)$   随t变化时C的变化\nEDM–$C_{in}$\n损失函数——相邻两个时间输出值差距最小化$\\mathcal{L}^N(\\theta)=\\mathbb{E}[|f_\\theta(\\mathbf{x}{t{n+1}},t_{n+1})-f_\\theta(\\hat{\\mathbf{x}}{t_n},t_n)|2^2]$  再经过EMA,最终$\\mathcal{L}^N(\\theta,\\theta^-)=\\mathbb{E}[|f\\theta(\\mathbf{x}{t_{n+1}},t_{n+1})-f_{\\theta^-}(\\hat{\\mathbf{x}}_{t_n},t_n)|_2^2]$\n一致性蒸馏（简称CD，Consistency Distillation）——从已经学好的score function蒸馏\n\n已经有了score function $\\mathbf{s}_{\\phi}(\\mathbf{x}(t),t)$\n一致性训练(简称CT，Consistency Training)——从数据中直接学\n\n用$\\nabla\\log p_t(\\mathbf{x}_t)=-\\mathbb{E}\\left[\\frac{\\mathbf{x}_t-\\mathbf{x}}{t^2}|\\mathbf{x}_t\\right]$来代替一致性蒸馏中的已有的sore fuction\n如何通过一致性模型采样获得图像\n一步采样\n给定一个$x_t$，带入一致性模型\n多步采样\n可提升图像质量\nSR3\nSR3 is an approach to image super resolution via iterative refinement\n通过迭代优化实现生成图像超分辨率\nkey words\n\niterative refinement\nboth faces and natural images\nbicubic interpolation\nflexibility inchoosing number of diffusion steps, and the noise schedule during inference\nFID\nrather than estimating the posterior mean, SR3 generates samples from the target posterior.\nconstant number of refinement steps (often no more than 100).\nonot requireanyauxiliaryobjective function inorder toensureconsistencywith the low resolutioninputs\nour diffusion models do not provide a knob to control sample quality vs. sample diversity（如何平衡样本质量与样本多样性吗？）, and finding ways to do so isinteresting\navenue for future research.\n\n涉及知识点\n\nscore matching\nLangevin dynamics\nPSNR and SSIM\nresidual blocks\n级联结构\nNormalizing flows\nanti-aliasing\nImageNet\nDropout\n\n总结\n\n将LR作为条件输入\n不在取离散的t，而是将同样范围内连续t的采样值（即noise）输入\n或许可以减小推理步数，加快速度？\n级联 分阶段生成：\n第一阶段：使用无条件生成模型（如DDPM）生成低分辨率图像（如64×64）。\n第二阶段：将低分辨率图像输入第一个SR3模型，进行4倍上采样（64→256）。\n第三阶段：将256×256图像输入第二个SR3模型，再次4倍上采样至1024×1024。\n\n问题\n\n下采样操作，将 HR 图像的尺⼨减半，⽣成对应的 LR 图像\n下采样方式如何选择（是否采用SR3论文中提到的双三次插值？），以及为什么规定LR为HR尺寸减半后的结果\nSR3采用的是迭代优化实现图像超分辨率重建的方法，是否面临计算和时间成本高的问题，如何解决是否可以参考连续一致性模型的做法\n连续一致性模型有单步采样和多部采样两种方式，多部采样可以理解为牺牲速度换取高质量？是否可以再次基础上实现超分辨率重建？\n尝试https://github.com/openai/consistency_models\n和https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement 时遇到困难\n对数学公式的推导要掌握到什么程度？\n\n","categories":["科研实习周记"]},{"title":"week2","url":"/2025/06/25/week2/","content":"diffusion model (李宏毅)笔记\n概念部分\n一般图像生成模型基本框架\ntext encoder → generation model → decoder\nFID(Frechet Inception Distance)\nFID是一种用于评估生成图像质量的度量标准\n\n\n特征提取  使用预训练的 Inception V3 模型（在 ImageNet 数据集上训练的图像分类网络）作为特征提取器。输入图像（通常调整为 299×299 的分辨率）会通过 Inception V3 前向传播，提取池化层（即 pool3 层）的输出特征。这个特征是一个 2048 维的向量。\n\n\n特征分布假设  FID 假设提取的特征向量服从多变量正态分布。对于真实图像集合X和生成图像集合G，分别计算特征的均值向量和协方差矩阵：\n真实图像特征均值 $\\mu_{r}$   协方差 $\\Sigma_{r}$\n生成图像特征均值 $\\mu_{g}$   协方差 $\\Sigma_{g}$\n\n\nFréchet 距离计算\nFréchet 距离用来衡量两个正态分布之间的差异$$\\mathrm{FID}=|\\mu_r-\\mu_g|_2^2+\\mathrm{Tr}(\\Sigma_r+\\Sigma_g-2(\\Sigma_r\\Sigma_g)^{1/2})$$\n第一项衡量两个分布均值的欧几里得距离，表示分布中心的偏移，第二项衡量协方差矩阵的差异，反映分布形状和分散度的不同\n\n\n原理部分\nhttps://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4\n\n\n最大似然估计 https://zhuanlan.zhihu.com/p/55791843\n\n\n\n扩散模型与能量模型，Score-Matching和SDE，ODE的关系 https://zhuanlan.zhihu.com/p/576779879\n\n\n疑问\n\n李宏毅认为噪声实际上不是一步一步加进$x_{0}$的,而是一步实现的\n\n但通过对一致性模型的学习，我了解到diffusion model的前向过程和逆向过程实际上都能表示为SDE过程，需要进行多次迭代，而consistency model就是为了解决这个问题，将SDE的随机项消除，转变为ODE过程，从而实现减少迭代次数，这是否与上图观点相悖？\n\n","categories":["科研实习周记"]},{"title":"week3","url":"/2025/06/30/week3/","content":"Few-Shot\nPhasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption\nhttps://arxiv.org/pdf/2309.03729\nAbstract\n\n当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节\n引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）\n跨领域情景的结构一致性\n\nChallenges\n\noverfit\n细节学习阶段（t较小的时候）风格迁移失败\n现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转\n\nMethod\nTraining with Phasic Content Fusion\n在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合$E(x^{A})$和噪声$z\\sim\\mathcal{N}(0,I)$，\n$\\hat{E}(x^A)=m(t)E(x^A)+(1-m(t))z$\n然后使用多个卷积块将 $\\hat{E}(x^A)$ 与 $E(x_{t}^{A})$ 融合，得到融合后的特征 $E(x^A,x_t^A)$ ，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的 $x_{t-1}^A$\n方向分布一致性损失函数 directional distribution consistency loss (DDC)\n最终的损失函数由以下三个损失函数构成：\n\n\nDirectional distribution consistency loss\n$\\mathcal{L}_{DDC}=|E(x^A)+w,E(x_0^{A\\to B})|^2$\n其中w为方向向量，给定源分布 $A={x_{1}^{A},\\cdots x_{m}^{A}}$ 和目标分布 $B={x_{1}^{B},\\cdots x_{m}^{B}}$ ,特征空间中从源域中心到目标域中心的跨域方向向量w,\n$w=\\frac{1}{m}\\sum_{i=1}^mE(x_i^B)-\\frac{1}{n}\\sum_{i=1}^nE(x_i^A)$\n\n\nStyle loss\n$\\mathcal{L}{style}=\\frac{1}{m}\\sum{i=1}^{m}\\sum_{l}w_{l}|G^{l}(x_{0}^{A\\to B})-G^{l}(x_{i}^{B})|^{2}$\n用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵\n\n\nDiffusion Loss\n\n\n$\\mathcal{L}{dif}=||\\epsilon\\theta(x_t^B,t)-\\epsilon||^2$\n\n\n最终的损失函数为：\n$\\mathcal{L}=m(t)(1-w(t))(\\lambda_{DDC}\\mathcal{L}{DDC}(x^{A},x{0}^{A\\to B})+\\lambda_{style}\\mathcal{L}{style}(x{0}^{A\\to B},x^{B}))+w(t)\\mathcal{L}_{dif}(x^{B})$\n迭代跨域结构引导 Iterative Cross-domain Structure Guidance(ICSG)\n需要进一步理解\n实验及评估过程\n相关概念\n图像翻译 Image-to-Image Translation\n将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。\nGram矩阵\n原理\nn维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，很明显，这是一个对称矩阵。\n输入图像的feature map为[ ch, h, w]。我们经过flatten（即是将h* w 进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w, ch]的矩阵。再对两个作内积得到Gram矩阵。\n应用\nGram matrix的应用-风格迁移：\n\n\n准备基准图像和风格图像\n\n\n使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature map）\n\n\n分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像\n\n\n一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。\n消融实验 Ablation Study\n类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。\nSpecialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style\nhttps://arxiv.org/pdf/2211.12572\n主要内容\n这篇论文提出Specialist Diffusion：相当于一个即插即用的微调工具包，包括文本到图像的定制数据增强，content loss to facilitate content-style disentanglement，sparsely updating diffusion time steps。主要适用于以少量已知风格的图片训练模型，使其能够通过特定的文本提示生成相应风格的图片。\nData Augmentations for Text2Image Diffusion（数据增强）\nImage Augmentation\nText Prompt Augmentation\nCaption Retrieval Augmentation 标题搜索增强\nSynonym Augmentation 同义词增强\nDoubled Augmentation 双重增强\nContent Loss\nSparse Updating\n相关概念\n增强泄露问题 augmentation leakage\n生成模型在训练过程中，会记住训练样本及其经过增强后的版本，这样在推理（生成新内容）阶段，就容易生成与训练时相似的图像。\n举个文中例子，很多旋转后的图像理论上算自然照片，但在真实自然图像集合里，它们出现的概率其实更低。要是训练时过度用旋转增强，模型就可能 “bias（偏向）” 生成更多带旋转的物体，可这些并非实际想要的（“un - tended” ，即不符合自然场景常见分布 ），相当于增强操作的影响 “泄漏” 到生成结果里，让生成内容偏离真实自然数据的合理分布，这就是 “augmentation leakage” 。简单说，就是数据增强的不当使用，让模型学到了增强带来的 “虚假模式”，而非真实场景的合理特征，影响生成效果。\n正则化 Regularization\n正则化是用来防止模型过拟合而采取的手段，对代价函数增加一个限制条件，限制其较高次的参数大小不能过大\n参考：https://blog.csdn.net/weixin_41960890/article/details/104891561\n一.LoRA原理\n参考：https://zhuanlan.zhihu.com/p/702629428\n原论文：https://arxiv.org/pdf/2106.09685\nLoRA(Low-Rank Adaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。\nLoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。\n1.1实现流程\n\n\n在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；\n用随机高斯分布初始化 A，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵 B；\n训练完成后，将 B 矩阵与 A 矩阵相乘后合并预训练模型参数作为微调后的模型参数。\n\n具体来讲，预训练权重矩阵 $\\mathbf{W}_0\\in\\mathbb{R}^{d\\times d}$ ，\n将增量参数矩阵 $\\Delta\\mathbf{W}$ ，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式\n$\\mathbf{W}{0}+\\Delta\\mathbf{W}=\\mathbf{W}{0}+\\mathbf{BA}$\n其中 $\\mathbf{B}\\in\\mathbb{R}^{d\\times r}$ ，$\\mathbf{A}\\in\\mathbb{R}^{r\\times d}$ ，秩$\\mathrm{r}$远小于$\\mathrm{d}$\n给定输入$.\\mathbf{x}\\in\\mathbb{R}^d$ ,添加LoRA后的输出$.\\mathbf{h}\\in\\mathbb{R}^d$\n$\\mathbf{h}=(\\mathbf{W}{0}+\\Delta\\mathbf{W})\\mathbf{x}=\\mathbf{W}{0}\\mathbf{x}+\\mathbf{BA}\\mathbf{x}$\n$\\Delta\\mathbf{h}=\\mathbf{BAx}$\n1.2LoRA参数合并系数\n实际实现时以以下形式合并，其中$\\alpha$为超参数\n$\\mathbf{h}=(\\mathbf{W}_{0}+\\frac{\\alpha}{r}\\Delta\\mathbf{W})\\mathbf{x}$\n系数$\\frac{\\alpha}{r}$越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合\n系数$\\frac{\\alpha}{r}$越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）\n一般来说，在给定任务上LoRA微调，让$\\alpha$为$\\mathbf{r}$的2倍数。\n1.3 LoRA的秩$\\mathbf{r}$如何选择\n目标：找到一个秩$\\mathbf{r}$，使$\\mathrm{BA}$无限接近$\\Delta\\mathbf{W}$的表达能力。\n秩$\\mathbf{r}$越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。\n","categories":["科研实习周记"]},{"title":"week4","url":"/2025/06/30/week4/","content":"一、LoRA微调\ntest1:\n将训练第二阶段注意力机制中的线性层’w_q’,‘w_k’,'w_v’替换为LoRA层\nlora_r: 8                    # LoRA秩大小lora_alpha: 16               # LoRA缩放因子\n预训练模型只加载第二阶段扩散网络权重\n结果： _IncompatibleKeys 错误（缺少 encoder/decoder 层）\n可能原因：只加载了第二阶段模型，缺少 Autoencoder 部分，无法将输入图像编码为潜在空间表示，无法将生成的潜在编码解码为图像\ntest2:\n由第一次尝试得，当只微调扩散网络本身时，仍需要\n\n\n先加载第一阶段Autoencoder\n\n\n再加载第二阶段扩散网络权重\n\n\n则test2总体流程为：\n\n加载完整的加载完整的扩散模型（含 Autoencoder + 扩散网络）\n注入 LoRA 参数到扩散网络的特定层\n冻结其他参数，随机选择20张图片训练 LoRA 层\n\n测试结果：\n\n\n\n评估指标\n微调前\n微调后\n变化趋势\n变化率\n\n\n\n\nNMSE\n0.006882\n0.005570\n↓\n-19.06%\n\n\nRMSE\n0.029842\n0.026012\n↓\n-12.83%\n\n\nSSIM\n0.945097\n0.955175\n↑\n+1.07%\n\n\nPSNR (dB)\n30.574329\n31.882632\n↑\n+4.28%\n\n\n\n注意力机制\n原论文：https://arxiv.org/pdf/1706.03762\n什么是注意力机制\n注意力机制就是让模型重点关注重要信息，忽略次要信息。注意力机制分为空间注意力和时间注意力，前者用于图像处理，后者用于自然语言处理.\n原理\nQuery：当前需要查询的目标，即当前输入的特征表示。\nKey：可以将每个单词的重要特征表示看作成 Key。\nValue：每个单词本身的特征向量看作为 Value，一般和 Key成对出现，也就是我们常说的&quot;键-值&quot;对。\n\n核心公式（原论文中）：\n$$\nAttention(Q,K,V)=Softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V\n$$\n步骤：\n\n\n先根据 Query，Key计算两者的相关性，然后再通过 softmax 函数得到 注意力分数，使用 softmax 函数是为了使得所有的注意力分数在 [0,1] 之间，并且和为1。\n相关性公式一般表示如下：\n$$\nscore(q,k_i)=softmax(\\alpha(q,k_i))=\\frac{exp(\\alpha(q,k_i))}{\\sum_1^jexp(\\alpha(q,k_j))}\n$$\n其中$\\alpha(q,k_{i})$有很多变体：\ne.g. 在加性注意力中\n$$\n\\alpha(q,k_i)=w_v^Ttanh(W_qq+W_kk)\n$$\nW_q：Query对应的可训练矩阵\nW_k: Key对应的可训练矩阵\nw_v^T: Value对应的可训练矩阵\n(tanh为双曲正切函数，作为一种常见的激活函数)\ne.g.在缩放点积注意力中\n$$\n\\alpha(q,k_i)=\\frac{QK^T}{\\sqrt{d}}\n$$\n其中d为Keys的维度大小，除以sqrt{d}是为了使方差变小，训练梯度更新时更稳定\n\n\n将注意力分数加权求和，得到带注意力分数的Value\n\n\n自注意力机制（Self-Attention Mechanism）\n\n2.1 Embedding 操作, 将向量x转化为a,a作为注意力机制的input data\n2.2 q, k 操作\n$$q^i=W^qa^i$$\n$$k^i=W^ka^i$$\n$$v^i=W^va^i$$\n多头注意力机制（Multi-head Self-Attention Machanism）\n参考：https://zhuanlan.zhihu.com/p/631398525\n通道注意力机制\n空间注意力机制\n","categories":["科研实习周记"]},{"title":"串口重定向","url":"/2025/07/08/%E4%B8%B2%E5%8F%A3%E9%87%8D%E5%AE%9A%E5%90%91/","content":"stm32串口重定向HAL库\n#include &lt;stdio.h&gt;// 包含标准输入输出头文件 int fputc(int ch,FILE *f)&#123;//采用轮询方式发送1字节数据，超时时间设置为无限等待HAL_UART_Transmit(&amp;huart1,(uint8_t *)&amp;ch,1,HAL_MAX_DELAY);return ch;&#125;int fgetc(FILE *f)&#123;uint8_t ch;// 采用轮询方式接收 1字节数据，超时时间设置为无限等待HAL_UART_Receive( &amp;huart1,(uint8_t*)&amp;ch,1, HAL_MAX_DELAY );return ch;&#125;","categories":["-单片机 -stm32"]},{"title":"使用循环缓冲区进行串口数据解析","url":"/2025/06/29/%E4%BD%BF%E7%94%A8%E5%BE%AA%E7%8E%AF%E7%BC%93%E5%86%B2%E5%8C%BA%E8%BF%9B%E8%A1%8C%E4%B8%B2%E5%8F%A3%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/","content":"stm32cubemx配置\n开启串口收发异步模式，开启串口空闲中断\n代码实现\n#include &quot;command.h&quot;// 指令的最小长度#define COMMAND_MIN_LENGTH 4// 循环缓冲区大小#define BUFFER_SIZE 128// 循环缓冲区uint8_t buffer[BUFFER_SIZE];// 循环缓冲区读索引uint8_t readIndex = 0;// 循环缓冲区写索引uint8_t writeIndex = 0;/*** @brief 增加读索引* @param length 要增加的长度*/void Command_AddReadIndex(uint8_t length) &#123;    readIndex += length;    readIndex %= BUFFER_SIZE;&#125;/*** @brief 读取第i位数据 超过缓存区长度自动循环* @param i 要读取的数据索引*/uint8_t Command_Read(uint8_t i) &#123;    uint8_t index = i % BUFFER_SIZE;    return buffer[index];&#125;/*** @brief 计算未处理的数据长度* @return 未处理的数据长度* @retval 0 缓冲区为空* @retval 1~BUFFER_SIZE-1 未处理的数据长度* @retval BUFFER_SIZE 缓冲区已满*///uint8_t Command_GetLength() &#123;//  // 读索引等于写索引时，缓冲区为空//  if (readIndex == writeIndex) &#123;//    return 0;//  &#125;//  // 如果缓冲区已满,返回BUFFER_SIZE//  if (writeIndex + 1 == readIndex || (writeIndex == BUFFER_SIZE - 1 &amp;&amp; readIndex == 0)) &#123;//    return BUFFER_SIZE;//  &#125;//  // 如果缓冲区未满,返回未处理的数据长度//  if (readIndex &lt;= writeIndex) &#123;//    return writeIndex - readIndex;//  &#125; else &#123;//    return BUFFER_SIZE - readIndex + writeIndex;//  &#125;//&#125;uint8_t Command_GetLength() &#123;    return (writeIndex + BUFFER_SIZE - readIndex) % BUFFER_SIZE;&#125;/*** @brief 计算缓冲区剩余空间* @return 剩余空间* @retval 0 缓冲区已满* @retval 1~BUFFER_SIZE-1 剩余空间* @retval BUFFER_SIZE 缓冲区为空*/uint8_t Command_GetRemain() &#123;    return BUFFER_SIZE - Command_GetLength();&#125;/*** @brief 向缓冲区写入数据* @param data 要写入的数据指针* @param length 要写入的数据长度* @return 写入的数据长度*/uint8_t Command_Write(uint8_t *data, uint8_t length) &#123;    // 如果缓冲区不足 则不写入数据 返回0    if (Command_GetRemain() &lt; length) &#123;        return 0;    &#125;    // 使用memcpy函数将数据写入缓冲区    if (writeIndex + length &lt; BUFFER_SIZE) &#123;        memcpy(buffer + writeIndex, data, length);        writeIndex += length;    &#125; else &#123;        uint8_t firstLength = BUFFER_SIZE - writeIndex;        memcpy(buffer + writeIndex, data, firstLength);        memcpy(buffer, data + firstLength, length - firstLength);        writeIndex = length - firstLength;    &#125;    return length;&#125;/*** @brief 尝试获取一条指令* @param command 指令存放指针* @return 获取的指令长度* @retval 0 没有获取到指令*/uint8_t Command_GetCommand(uint8_t *command) &#123;    // 寻找完整指令    while (1) &#123;        // 如果缓冲区长度小于COMMAND_MIN_LENGTH 则不可能有完整的指令        if (Command_GetLength() &lt; COMMAND_MIN_LENGTH) &#123;        return 0;        &#125;        // 如果不是包头 则跳过 重新开始寻找        if (Command_Read(readIndex) != 0xAA) &#123;        Command_AddReadIndex(1);        continue;        &#125;        // 如果缓冲区长度小于指令长度 则不可能有完整的指令        uint8_t length = Command_Read(readIndex + 1);        if (Command_GetLength() &lt; length) &#123;        return 0;        &#125;        // 如果校验和不正确 则跳过 重新开始寻找        uint8_t sum = 0;        for (uint8_t i = 0; i &lt; length - 1; i++) &#123;        sum += Command_Read(readIndex + i);        &#125;        if (sum != Command_Read(readIndex + length - 1)) &#123;        Command_AddReadIndex(1);        continue;        &#125;        // 如果找到完整指令 则将指令写入command 返回指令长度        for (uint8_t i = 0; i &lt; length; i++) &#123;        command[i] = Command_Read(readIndex + i);        &#125;        Command_AddReadIndex(length);        return length;    &#125;&#125;\n头文件：\n#ifndef INC_COMMAND_H_#define INC_COMMAND_H_#include &quot;main.h&quot;#include &lt;string.h&gt;uint8_t Command_Write(uint8_t *data, uint8_t length);uint8_t Command_GetCommand(uint8_t *command);#endif /* INC_COMMAND_H_ */\nmain.c:\n/* Private define ------------------------------------------------------------*//* USER CODE BEGIN PD */uint8_t readBuffer[10];/* USER CODE END PD */\n/* USER CODE BEGIN 0 */void HAL_UARTEx_RxEventCallback(UART_HandleTypeDef *huart, uint16_t Size)&#123;\tif (huart == &amp;huart2)&#123;\t\tCommand_Write(readBuffer, Size);\t\tHAL_UARTEx_ReceiveToIdle_IT(&amp;huart2, readBuffer, sizeof(readBuffer));\t&#125;&#125;/* USER CODE END 0 */\n/* USER CODE BEGIN 2 */HAL_UARTEx_ReceiveToIdle_IT(&amp;huart2, readBuffer, sizeof(readBuffer));uint8_t command[50];int commandLength = 0;/* USER CODE END 2 *//* Infinite loop *//* USER CODE BEGIN WHILE */while (1)&#123;    commandLength = Command_GetCommand(command);    if (commandLength != 0)&#123;        HAL_UART_Transmit(&amp;huart2, command, commandLength, HAL_MAX_DELAY);        for (int i = 2; i &lt; commandLength - 1; i += 2)&#123;            GPIO_PinState state = GPIO_PIN_SET;            if (command[i + 1] == 0x00)&#123;                state = GPIO_PIN_RESET;            &#125;            if (command[i] == 0x01)&#123;                HAL_GPIO_WritePin(RED_GPIO_Port, RED_Pin, state);            &#125;else if (command[i] == 0x02)&#123;                HAL_GPIO_WritePin(GREEN_GPIO_Port, GREEN_Pin, state);            &#125;else if (command[i] == 0x03)&#123;                HAL_GPIO_WritePin(BLUE_GPIO_Port, BLUE_Pin, state);            &#125;        &#125;    &#125;/* USER CODE END WHILE *//* USER CODE BEGIN 3 */&#125;\n","categories":["-单片机 -stm32"]},{"title":"week5","url":"/2025/07/13/week5/","content":"一、论文阅读笔记\r\nFew-shot Image Generation with Diffusion Models\r\n论文：https://arxiv.org/pdf/2211.03264\r\n主要内容：\r\n提出Few-shot Diffusion Models (FDM)，在仅使用 10\r\n张训练图像时，就能生成具有合理多样性和质量的图像。\r\n主要挑战\r\n过拟合和模式崩溃（模型只能生成训练集中见过的少数几种样本）\r\n核心方法\r\n结构感知数据增强\r\n(Structure-Aware Data Augmentation)\r\n使用预训练的CLIP模型提取图像的语义特征，然后基于这些特征计算图像之间的相似性，\r\nCLIP相似度计算：$s_{ij}=\\frac{\\phi(I_i)\\cdot\\phi(I_j)}{\\|\\phi(I_i)\\|\\|\\phi(I_j)\\|}$\r\n其中 ϕ(⋅) 是CLIP图像编码器，sij ∈ [−1, 1]表示图像\r\nIi 和\r\nIj\r\n的语义相似度\r\n相似度高的图像对：应用更强的几何变换（如大幅旋转、裁剪），能提供更多样的“视角”而不会完全破坏结构。\r\n相似度低的图像对：应用较弱的变换，避免破坏其各自独特的结构信息。\r\n自适应变换强度: λij = λmin + (λmax − λmin)⋯ij\r\nλmin和λmax是最小/最大变换强度，相似度越高，变换强度越大\r\n层级优化机制\r\n(Hierarchical Optimization)\r\nFDM 将扩散模型（如DDPM）的UNet结构划分为两个层级：\r\n\r\n基础层：负责捕捉图像的全局结构和基本语义（一般是UNet的深层/瓶颈层）\r\n细节层：负责生成图像的局部细节（一般是UNet的浅层）\r\n\r\n采用交替优化策略： 阶段一 (Freeze Detail\r\nLayers)：冻结西接层参数，只优化基础层 阶段二 (Freeze Base\r\nLayers)：冻结基础层参数，只优化细节层 ### 自适应卷积模块 (Adaptive\r\nConvolution Module)\r\n引入自适应卷积模块，根据输入特征图动态生成卷积核的偏移量 (offset)\r\n和调制标量 (modulation scalar) 偏移量：\r\n卷积核的采样位置根据输入内容进行微调\r\nΔpk = foffset(F; ϕ)\r\nfoffset是轻量子网络，F是输入特征图\r\n调制标量： 动态调整卷积核的权重，增强模型对输入变化的适应能力\r\nmk = σ(gmod(F; ψ))\r\no\r\n是sigmoid激活函数，限制输出在(0,1)范围\r\n自适应卷积公式： $\\mathbf{y}(p)=\\sum_{k=1}^K\\mathbf{w}_k\\cdot\\mathbf{F}(p+p_k+\\Delta\r\np_k)\\cdot m_k$\r\n模式崩溃\r\n对于某一个训练数据集，其中样本的概率分布为一个简单的一维高斯混合分布，包含两个峰，如下图\r\n\r\n模式崩溃问题是针对于生成样本的多样性，即生成的样本大量重复类似，如下图\r\n\r\n虽然生成样本的质量比较高，但是生成器完全没有捕捉到右边的峰的模式。\r\n解决思路：\r\n参考：https://cloud.tencent.com/developer/article/1522756\r\n二、代码改进\r\n改进代码，\r\n（1）由原来只替换注意力层改进为替换UNet的所有线性层和1x1卷积层\r\n（2）改进数据集的选择，原来是随机挑选了10个样本训练lora参数，应改进为挑选部分dpm和少量的irt4_car样本去训练\r\n7.9遇到的问题：替换Swin Transformer中的层，需要访问weight属性\r\n尝试解决 （1）：跳过Swin Transformer中的层 （2）：当win\r\nTransformer访问qkv.weight时，返回原始层的权重\r\n7.12 解决了LoRA注入失败的问题 下一步：测试训练效果\r\n改进用于微调的样本的选择\r\n","categories":["科研实习周记"]},{"title":"注意力机制","url":"/2025/07/02/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","content":"注意力机制\n原论文：https://arxiv.org/pdf/1706.03762\n什么是注意力机制\n注意力机制就是让模型重点关注重要信息，忽略次要信息。注意力机制分为空间注意力和时间注意力，前者用于图像处理，后者用于自然语言处理.\n原理\nQuery：当前需要查询的目标，即当前输入的特征表示。\nKey：可以将每个单词的重要特征表示看作成 Key。\nValue：每个单词本身的特征向量看作为 Value，一般和 Key成对出现，也就是我们常说的&quot;键-值&quot;对。\n\n核心公式（原论文中）：\n$Attention(Q,K,V)=Softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$\n步骤：\n\n\n先根据 Query，Key计算两者的相关性，然后再通过 softmax 函数得到 注意力分数，使用 softmax 函数是为了使得所有的注意力分数在 [0,1] 之间，并且和为1。\n相关性公式一般表示如下：\n$score(q,k_i)=softmax(\\alpha(q,k_i))=\\frac{exp(\\alpha(q,k_i))}{\\sum_1^jexp(\\alpha(q,k_j))}$\n其中$\\alpha(q,k_{i})$有很多变体：\ne.g. 在加性注意力中\n$\\alpha(q,k_i)=w_v^Ttanh(W_qq+W_kk)$\nW_q：Query对应的可训练矩阵\nW_k: Key对应的可训练矩阵\nw_v^T: Value对应的可训练矩阵\n(tanh为双曲正切函数，作为一种常见的激活函数)\ne.g.在缩放点积注意力中\n$\\alpha(q,k_i)=\\frac{QK^T}{\\sqrt{d}}$\n其中d为Keys的维度大小，除以sqrt{d}是为了使方差变小，训练梯度更新时更稳定\n\n\n将注意力分数加权求和，得到带注意力分数的Value\n\n\n自注意力机制（Self-Attention Mechanism）\n\n2.1 Embedding 操作, 将向量x转化为a,a作为注意力机制的input data\n2.2 q, k 操作\n$q^i=W^qa^i$\n$k^i=W^ka^i$\n$v^i=W^va^i$\n一般用 $\\alpha_{1,i}=q^1\\cdot k^i/\\sqrt{d}$  表示 $a^{1}$ 与 $a^{i}$ 之间的关系，\n其中$\\mathrm{d}$表示 $\\mathrm{q}$和$\\mathrm{k}$ 矩阵的维度\n\n2.3 v操作\n$b^1=\\sum_i\\tilde{\\alpha}_{1,i}v^i$\n$b^2=\\sum_i\\tilde{\\alpha}_{2,i}v^i$\n以此类推\n\n多头注意力机制\nq,k操作\n这里以两头为例\n$q^{i,1}=W^{q,1}q^i$\n$q^{i,2}=W^{q,2}q^i$\nk也是同样的操作\nv操作\n与自注意力机制相同\n\n通道注意力机制\n通道注意力机制是通过计算每个通道channel的重要性程度；因此，常常被用在卷积神经网络里面。目前，比较经典的通道注意力机制方法就是SENet模型，SENet通过学习通道间的关系（每个通道的重要性），提升了网络在特征表示中的表达能力，进而提升了模型的性能。\nSENet介绍\n参考：https://zhuanlan.zhihu.com/p/631398525\n空间注意力机制\n通过引入注意力模块，使模型能够自适应地学习不同区域的注意力权重。这样，模型可以更加关注重要的图像区域，而忽略不重要的区域。其中，最为典型的是 CBAM（Convolutional Block Attention Module），CBAM 是一种结合了通道注意力和空间注意力的模型，旨在增强卷积神经网络对图像的关注能力。\n","categories":["-科研实习 -深度学习"]},{"title":"调制与解调","url":"/2025/07/13/%E8%B0%83%E5%88%B6%E4%B8%8E%E8%A7%A3%E8%B0%83/","content":"\r\n"}]