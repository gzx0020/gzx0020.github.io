[{"title":"Specialist Diffusion","url":"/2025/06/26/Specialist-Diffusion/","content":"Specialist\r\nDiffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image\r\nDiffusion Models to Learn Any Unseen Style\r\nhttps://arxiv.org/pdf/2211.12572\r\n主要内容\r\n这篇论文提出Specialist\r\nDiffusion：相当于一个即插即用的微调工具包，包括文本到图像的定制数据增强，content\r\nloss to facilitate content-style disentanglement，sparsely updating\r\ndiffusion time\r\nsteps。主要适用于以少量已知风格的图片训练模型，使其能够通过特定的文本提示生成相应风格的图片。\r\nData\r\nAugmentations for Text2Image Diffusion（数据增强）\r\nImage Augmentation\r\nText Prompt Augmentation\r\nCaption Retrieval\r\nAugmentation 标题搜索增强\r\nSynonym Augmentation\r\n同义词增强\r\nDoubled Augmentation\r\n双重增强\r\nContent Loss\r\nSparse Updating\r\n相关概念\r\n增强泄露问题 augmentation\r\nleakage\r\n生成模型在训练过程中，会记住训练样本及其经过增强后的版本，这样在推理（生成新内容）阶段，就容易生成与训练时相似的图像。\r\n举个文中例子，很多旋转后的图像理论上算自然照片，但在真实自然图像集合里，它们出现的概率其实更低。要是训练时过度用旋转增强，模型就可能\r\n“bias（偏向）” 生成更多带旋转的物体，可这些并非实际想要的（“un - tended”\r\n，即不符合自然场景常见分布 ），相当于增强操作的影响 “泄漏”\r\n到生成结果里，让生成内容偏离真实自然数据的合理分布，这就是 “augmentation\r\nleakage” 。简单说，就是数据增强的不当使用，让模型学到了增强带来的\r\n“虚假模式”，而非真实场景的合理特征，影响生成效果。\r\n正则化 Regularization\r\n正则化是用来防止模型过拟合而采取的手段，对代价函数增加一个限制条件，限制其较高次的参数大小不能过大\r\n参考：https://blog.csdn.net/weixin_41960890/article/details/104891561\r\n","categories":["-科研实习 -RadioDiff微调 -Few-Shot相关论文阅读笔记"]},{"title":"Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption","url":"/2025/06/26/Phasic%20Content%20Fusing%20Diffusion%20Model%20with%20Directional%20Distribution%20Consistency%20for%20Few-Shot%20Model%20Adaption/","content":"Few-Shot\r\nPhasic\r\nContent Fusing Diffusion Model with Directional Distribution Consistency\r\nfor Few-Shot Model Adaption\r\nAbstract\r\n\r\n当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节\r\n引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）\r\n跨领域情景的结构一致性\r\n\r\nChallenges\r\n\r\noverfit\r\n细节学习阶段（t较小的时候）风格迁移失败\r\n现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转\r\n\r\nMethod\r\nTraining with Phasic\r\nContent Fusion\r\n在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合E(xA)和噪声z ∼ 𝒩(0, I)， Ê(xA) = m(t)E(xA) + (1 − m(t))z\r\n然后使用多个卷积块将 Ê(xA)\r\n与 E(xtA)\r\n融合，得到融合后的特征 E(xA, xtA)\r\n，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的\r\nxt − 1A\r\n#### 方向分布一致性损失函数 directional distribution consistency loss\r\n(DDC) 最终的损失函数由以下三个损失函数构成：\r\n\r\nDirectional distribution consistency loss\r\nℒDDC = ∥E(xA) + w, E(x0A → B)∥2\r\n其中w为方向向量，给定源分布 A = {x1A, ⋯xmA}\r\n和目标分布 B = {x1B, ⋯xmB}\r\n,特征空间中从源域中心到目标域中心的跨域方向向量w,\r\n$w=\\frac{1}{m}\\sum_{i=1}^mE(x_i^B)-\\frac{1}{n}\\sum_{i=1}^nE(x_i^A)$\r\nStyle loss\r\n$\\mathcal{L}_{style}=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{l}w_{l}\\|G^{l}(x_{0}^{A\\to\r\nB})-G^{l}(x_{i}^{B})\\|^{2}$\r\n用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵\r\nDiffusion Loss\r\nℒdif = ||ϵθ(xtB, t) − ϵ||2\r\n\r\n最终的损失函数为：\r\nℒ = m(t)(1 − w(t))(λDDCℒDDC(xA, x0A → B) + λstyleℒstyle(x0A → B, xB)) + w(t)ℒdif(xB)\r\n迭代跨域结构引导\r\nIterative Cross-domain Structure Guidance(ICSG)\r\n需要进一步理解\r\n实验及评估过程\r\n相关概念\r\n图像翻译 Image-to-Image\r\nTranslation\r\n将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。\r\nGram矩阵\r\n原理\r\nn维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram\r\nmatrix)，很明显，这是一个对称矩阵。 输入图像的feature map为[ ch, h,\r\nw]。我们经过flatten（即是将h* w\r\n进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w,\r\nch]的矩阵。再对两个作内积得到Gram矩阵。\r\n应用\r\nGram matrix的应用-风格迁移： 1. 准备基准图像和风格图像\r\n\r\n使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature\r\nmap）\r\n分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像\r\n\r\n一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。\r\n消融实验 Ablation Study\r\n类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。\r\n","categories":["-科研实习 -RadioDiff微调 -Few-Shot相关论文阅读笔记"]},{"title":"week1","url":"/2025/05/20/week1/","content":"一致性模型（Consistency\r\nModels，CM）\r\nhttps://zhuanlan.zhihu.com/p/623402026\r\n一致性模型（Consistency\r\nModels，CM）主要解决扩散生成模型迭代采样过程缓慢的问题，支持一步采样快速生成和多步采样高精度生成，CM\r\n的本质就是将任何时间步的点映射到轨迹的起点。CM 的一个关键的性质是\r\nself-consistency 性：相同轨迹上的点映射到相同的初始点。\r\nSDE与ODE\r\n前向过程满足的SDE： $=(,t)t+g(t)(t) $ f:漂移因子 g:扩散因子\r\nw:维纳过程(标准布朗运动) score:∇xlog p(x)\r\n即概率密度对数的梯度\r\n朗之万动力学\r\n边缘概率密度\r\nscore matching\r\n逆向过程的SDE为： $\\mathrm{d}\\mathbf{x}=[\\mathbf{f}(\\mathbf{x},t)-g^2(t)\\nabla_\\mathbf{x}\\log\r\np_t(\\mathbf{x})]\\mathrm{d}t+g(t)\\mathrm{d}\\bar \\\\\r\n{\\mathbf{w}}(t)$\r\nODE：SDE去掉维纳过程，变成一个常微分方程\r\n$\\mathrm{d}\\mathbf{x}_t=\r\n\\begin{bmatrix}\r\nf(\\mathbf{x}_t,t)-\\frac{1}{2}g^2(t)\\nabla\\log p_t(\\mathbf{x}_t)\r\n\\end{bmatrix}\\mathrm{d}t$\r\n如何用神经网络训练一致性模型\r\n一致性函数 $f(\\mathbf{x}_t,t)=\r\n\\begin{cases}\r\n\\mathbf{x}_\\varepsilon, &amp; t=\\varepsilon \\\\\r\nf(\\mathbf{x}_{t^{\\prime}},t^{\\prime}), &amp; t\\in(\\varepsilon,T],\\forall\r\nt^{\\prime}\\in[\\varepsilon,T] &amp;\r\n\\end{cases}$\r\n一致性模型：即用神经网络模拟一致性函数的特性\r\n给定任意神经网络F, fθ(xt, t) = Cskip(t)xt + Cout(t)Fθ(xt, t)\r\n随t变化时C的变化\r\nEDM–Cin\r\n损失函数——相邻两个时间输出值差距最小化$\\mathcal{L}^N(\\theta)=\\mathbb{E}[\\|f_\\theta(\\mathbf{x}_{t_{n+1}},t_{n+1})-f_\\theta(\\hat{\\mathbf{x}}_{t_n},t_n)\\|_2^2]$\r\n再经过EMA,最终$\\mathcal{L}^N(\\theta,\\theta^-)=\\mathbb{E}[\\|f_\\theta(\\mathbf{x}_{t_{n+1}},t_{n+1})-f_{\\theta^-}(\\hat{\\mathbf{x}}_{t_n},t_n)\\|_2^2]$\r\n一致性蒸馏（简称CD，Consistency\r\nDistillation）——从已经学好的score function蒸馏\r\n\r\n已经有了score function sϕ(x(t), t)\r\n### 一致性训练(简称CT，Consistency Training)——从数据中直接学 \r\n用$\\nabla\\log\r\np_t(\\mathbf{x}_t)=-\\mathbb{E}\\left[\\frac{\\mathbf{x}_t-\\mathbf{x}}{t^2}|\\mathbf{x}_t\\right]$来代替一致性蒸馏中的已有的sore\r\nfuction\r\n如何通过一致性模型采样获得图像\r\n一步采样\r\n给定一个xt，带入一致性模型\r\n多步采样\r\n可提升图像质量\r\nSR3\r\nSR3 is an approach to image super resolution via iterative refinement\r\n通过迭代优化实现生成图像超分辨率\r\nkey words\r\n\r\niterative refinement\r\nboth faces and natural images\r\nbicubic interpolation\r\nflexibility inchoosing number of diffusion steps, and the noise\r\nschedule during inference\r\nFID\r\nrather than estimating the posterior mean, SR3 generates samples\r\nfrom the target posterior.\r\nconstant number of refinement steps (often no more than 100).\r\nonot requireanyauxiliaryobjective function inorder\r\ntoensureconsistencywith the low resolutioninputs\r\nour diffusion models do not provide a knob to control sample quality\r\nvs. sample diversity（如何平衡样本质量与样本多样性吗？）, and finding\r\nways to do so isinteresting avenue for future research.\r\n\r\n涉及知识点\r\n\r\nscore matching\r\nLangevin dynamics\r\nPSNR and SSIM\r\nresidual blocks\r\n级联结构\r\nNormalizing flows\r\nanti-aliasing\r\nImageNet\r\nDropout\r\n\r\n总结\r\n\r\n将LR作为条件输入\r\n不在取离散的t，而是将同样范围内连续t的采样值（即noise）输入\r\n或许可以减小推理步数，加快速度？\r\n级联 分阶段生成：\r\n第一阶段：使用无条件生成模型（如DDPM）生成低分辨率图像（如64×64）。\r\n第二阶段：将低分辨率图像输入第一个SR3模型，进行4倍上采样（64→256）。\r\n第三阶段：将256×256图像输入第二个SR3模型，再次4倍上采样至1024×1024。\r\n\r\n问题\r\n\r\n下采样操作，将 HR 图像的尺⼨减半，⽣成对应的 LR 图像\r\n下采样方式如何选择（是否采用SR3论文中提到的双三次插值？），以及为什么规定LR为HR尺寸减半后的结果\r\nSR3采用的是迭代优化实现图像超分辨率重建的方法，是否面临计算和时间成本高的问题，如何解决是否可以参考连续一致性模型的做法\r\n连续一致性模型有单步采样和多部采样两种方式，多部采样可以理解为牺牲速度换取高质量？是否可以再次基础上实现超分辨率重建？\r\n尝试https://github.com/openai/consistency_models 和https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement\r\n时遇到困难\r\n对数学公式的推导要掌握到什么程度？\r\n\r\n","categories":["-科研实习 -周记"]},{"title":"LoRA","url":"/2025/06/26/LoRA/","content":"一.LoRA原理\r\n参考：https://zhuanlan.zhihu.com/p/702629428 原论文：https://arxiv.org/pdf/2106.09685 LoRA(Low-Rank\r\nAdaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。\r\nLoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。\r\n1.1实现流程\r\n\r\n\r\n在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；\r\n\r\n用随机高斯分布初始化\r\nA，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵\r\nB；\r\n训练完成后，将 B 矩阵与 A\r\n矩阵相乘后合并预训练模型参数作为微调后的模型参数。\r\n\r\n具体来讲，预训练权重矩阵 W0 ∈ ℝd × d\r\n，\r\n将增量参数矩阵 ΔW\r\n，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式\r\nW0 + ΔW = W0 + BA\r\n其中 B ∈ ℝd × r\r\n，A ∈ ℝr × d\r\n，秩r远小于d\r\n给定输入.x ∈ ℝd\r\n,添加LoRA后的输出.h ∈ ℝd\r\nh = (W0 + ΔW)x = W0x + BAx\r\nΔh = BAx\r\n1.2LoRA参数合并系数\r\n实际实现时以以下形式合并，其中α为超参数\r\n$$\\mathbf{h}=(\\mathbf{W}_{0}+\\frac{\\alpha}{r}\\Delta\\mathbf{W})\\mathbf{x}$$\r\n系数$\\frac{\\alpha}{r}$越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合\r\n系数$\\frac{\\alpha}{r}$越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）\r\n一般来说，在给定任务上LoRA微调，让α为r的2倍数。\r\n1.3 LoRA的秩r如何选择\r\n目标：找到一个秩r，使BA无限接近ΔW的表达能力。\r\n秩r越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。\r\n","categories":["-科研实习 -RadioDiff微调"]},{"title":"week4","url":"/2025/06/30/week4/","content":"一、LoRA微调\r\ntest1:\r\n将训练第二阶段注意力机制中的线性层’w_q’,‘w_k’,’w_v’替换为LoRA层\r\nlora_r: 8                    # LoRA秩大小lora_alpha: 16               # LoRA缩放因子 预训练模型只加载第二阶段扩散网络权重\r\n结果： _IncompatibleKeys 错误（缺少 encoder/decoder 层）\r\n可能原因：只加载了第二阶段模型，缺少 Autoencoder\r\n部分，无法将输入图像编码为潜在空间表示，无法将生成的潜在编码解码为图像\r\ntest2:\r\n由第一次尝试得，当只微调扩散网络本身时，仍需要\r\n\r\n先加载第一阶段Autoencoder\r\n再加载第二阶段扩散网络权重\r\n\r\n则test2总体流程为：\r\n\r\n加载完整的加载完整的扩散模型（含 Autoencoder + 扩散网络）\r\n注入 LoRA 参数到扩散网络的特定层\r\n冻结其他参数，随机选择20张图片训练 LoRA 层\r\n\r\n测试结果：\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n评估指标\r\n微调前\r\n微调后\r\n变化趋势\r\n变化率\r\n\r\n\r\n\r\n\r\nNMSE\r\n0.006882\r\n0.005570\r\n↓\r\n-19.06%\r\n\r\n\r\nRMSE\r\n0.029842\r\n0.026012\r\n↓\r\n-12.83%\r\n\r\n\r\nSSIM\r\n0.945097\r\n0.955175\r\n↑\r\n+1.07%\r\n\r\n\r\nPSNR (dB)\r\n30.574329\r\n31.882632\r\n↑\r\n+4.28%\r\n\r\n\r\n\r\n注意力机制\r\n原论文：https://arxiv.org/pdf/1706.03762\r\n什么是注意力机制\r\n注意力机制就是让模型重点关注重要信息，忽略次要信息。注意力机制分为空间注意力和时间注意力，前者用于图像处理，后者用于自然语言处理.\r\n原理\r\nQuery：当前需要查询的目标，即当前输入的特征表示。\r\nKey：可以将每个单词的重要特征表示看作成 Key。\r\nValue：每个单词本身的特征向量看作为 Value，一般和\r\nKey成对出现，也就是我们常说的”键-值”对。\r\n\r\n核心公式（原论文中）：\r\n$$\r\nAttention(Q,K,V)=Softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V\r\n$$\r\n步骤：\r\n\r\n先根据 Query，Key计算两者的相关性，然后再通过 softmax 函数得到\r\n注意力分数，使用 softmax 函数是为了使得所有的注意力分数在 [0,1]\r\n之间，并且和为1。\r\n相关性公式一般表示如下：\r\n$$\r\nscore(q,k_i)=softmax(\\alpha(q,k_i))=\\frac{exp(\\alpha(q,k_i))}{\\sum_1^jexp(\\alpha(q,k_j))}\r\n$$\r\n其中α(q, ki)有很多变体：\r\ne.g. 在加性注意力中\r\nα(q, ki) = wvTtanh(Wqq + Wkk)\r\nW_q：Query对应的可训练矩阵\r\nW_k: Key对应的可训练矩阵\r\nw_v^T: Value对应的可训练矩阵\r\n(tanh为双曲正切函数，作为一种常见的激活函数)\r\ne.g.在缩放点积注意力中\r\n$$\r\n\\alpha(q,k_i)=\\frac{QK^T}{\\sqrt{d}}\r\n$$\r\n其中d为Keys的维度大小，除以sqrt{d}是为了使方差变小，训练梯度更新时更稳定\r\n将注意力分数加权求和，得到带注意力分数的Value\r\n\r\n自注意力机制（Self-Attention\r\nMechanism）\r\n\r\n2.1 Embedding 操作, 将向量x转化为a,a作为注意力机制的input data\r\n2.2 q, k 操作\r\nqi = Wqai\r\nki = Wkai\r\nvi = Wvai\r\n多头注意力机制（Multi-head\r\nSelf-Attention Machanism）\r\n参考：https://zhuanlan.zhihu.com/p/631398525 ##\r\n通道注意力机制\r\n空间注意力机制\r\n","categories":["-科研实习 -周记"]},{"title":"week3","url":"/2025/06/30/week3/","content":"Few-Shot\r\nPhasic\r\nContent Fusing Diffusion Model with Directional Distribution Consistency\r\nfor Few-Shot Model Adaption\r\nhttps://arxiv.org/pdf/2309.03729\r\nAbstract\r\n\r\n当t较大时学习目标域内容和风格信息，当t较小时学习目标域的局部细节\r\n引入一种新的方向分布一致性损失，确保生成分布和原分布之间的一致性，防止过拟合（overfit）\r\n跨领域情景的结构一致性\r\n\r\nChallenges\r\n\r\noverfit\r\n细节学习阶段（t较小的时候）风格迁移失败\r\n现有的少样本GAN适应只约束对应点成对距离（相对位置关系），无法约束分布旋转\r\n\r\nMethod\r\nTraining with Phasic\r\nContent Fusion\r\n在前向加噪过程中学习内容和风格信息，引入权重函数m(t),自适应地融合E(xA)和噪声z ∼ 𝒩(0, I)，\r\nÊ(xA) = m(t)E(xA) + (1 − m(t))z\r\n然后使用多个卷积块将 Ê(xA)\r\n与 E(xtA)\r\n融合，得到融合后的特征 E(xA, xtA)\r\n，最后将融合后的特征送入UNet解码器对噪声进行预测，得到包含增强内容信息的\r\nxt − 1A\r\n方向分布一致性损失函数\r\ndirectional distribution consistency loss (DDC)\r\n最终的损失函数由以下三个损失函数构成：\r\n\r\nDirectional distribution consistency loss\r\nℒDDC = ∥E(xA) + w, E(x0A → B)∥2\r\n其中w为方向向量，给定源分布 A = {x1A, ⋯xmA}\r\n和目标分布 B = {x1B, ⋯xmB}\r\n,特征空间中从源域中心到目标域中心的跨域方向向量w,\r\n$w=\\frac{1}{m}\\sum_{i=1}^mE(x_i^B)-\\frac{1}{n}\\sum_{i=1}^nE(x_i^A)$\r\nStyle loss\r\n$\\mathcal{L}_{style}=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{l}w_{l}\\|G^{l}(x_{0}^{A\\to\r\nB})-G^{l}(x_{i}^{B})\\|^{2}$\r\n用于计算生成图像和目标图像之间的分割损失，基于Gram矩阵\r\nDiffusion Loss\r\nℒdif = ||ϵθ(xtB, t) − ϵ||2\r\n\r\n最终的损失函数为：\r\nℒ = m(t)(1 − w(t))(λDDCℒDDC(xA, x0A → B) + λstyleℒstyle(x0A → B, xB)) + w(t)ℒdif(xB)\r\n迭代跨域结构引导\r\nIterative Cross-domain Structure Guidance(ICSG)\r\n需要进一步理解\r\n实验及评估过程\r\n相关概念\r\n图像翻译 Image-to-Image\r\nTranslation\r\n将图像中内容从一个图像域Ｘ转换到另一个图像域Ｙ，可以看作是将原始图像的某种属性Ｘ移除，重新赋予其新的属性Ｙ，也即是图像间的跨域转换。\r\nGram矩阵\r\n原理\r\nn维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram\r\nmatrix)，很明显，这是一个对称矩阵。 输入图像的feature map为[ ch, h,\r\nw]。我们经过flatten（即是将h* w\r\n进行平铺成一维向量）和矩阵转置操作，可以变形为[ ch, h* w]和[ h*w,\r\nch]的矩阵。再对两个作内积得到Gram矩阵。 ##### 应用\r\nGram matrix的应用-风格迁移：\r\n\r\n准备基准图像和风格图像\r\n使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature\r\nmap）\r\n分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像\r\n\r\n一般来说浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息。这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以把图像特征之间隐藏的联系提取出来，也就是各个特征之间的相关性高低。\r\n消融实验 Ablation Study\r\n类似于“控制变量法”，逐一控制参数来观察结果的变化，以确定不同参数对模型的影响。\r\nSpecialist\r\nDiffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image\r\nDiffusion Models to Learn Any Unseen Style\r\nhttps://arxiv.org/pdf/2211.12572\r\n主要内容\r\n这篇论文提出Specialist\r\nDiffusion：相当于一个即插即用的微调工具包，包括文本到图像的定制数据增强，content\r\nloss to facilitate content-style disentanglement，sparsely updating\r\ndiffusion time\r\nsteps。主要适用于以少量已知风格的图片训练模型，使其能够通过特定的文本提示生成相应风格的图片。\r\nData\r\nAugmentations for Text2Image Diffusion（数据增强）\r\nImage Augmentation\r\nText Prompt Augmentation\r\nCaption Retrieval\r\nAugmentation 标题搜索增强\r\nSynonym Augmentation\r\n同义词增强\r\nDoubled Augmentation\r\n双重增强\r\nContent Loss\r\nSparse Updating\r\n相关概念\r\n增强泄露问题 augmentation\r\nleakage\r\n生成模型在训练过程中，会记住训练样本及其经过增强后的版本，这样在推理（生成新内容）阶段，就容易生成与训练时相似的图像。\r\n举个文中例子，很多旋转后的图像理论上算自然照片，但在真实自然图像集合里，它们出现的概率其实更低。要是训练时过度用旋转增强，模型就可能\r\n“bias（偏向）” 生成更多带旋转的物体，可这些并非实际想要的（“un - tended”\r\n，即不符合自然场景常见分布 ），相当于增强操作的影响 “泄漏”\r\n到生成结果里，让生成内容偏离真实自然数据的合理分布，这就是 “augmentation\r\nleakage” 。简单说，就是数据增强的不当使用，让模型学到了增强带来的\r\n“虚假模式”，而非真实场景的合理特征，影响生成效果。\r\n正则化 Regularization\r\n正则化是用来防止模型过拟合而采取的手段，对代价函数增加一个限制条件，限制其较高次的参数大小不能过大\r\n参考：https://blog.csdn.net/weixin_41960890/article/details/104891561\r\n一.LoRA原理\r\n参考：https://zhuanlan.zhihu.com/p/702629428 原论文：https://arxiv.org/pdf/2106.09685 LoRA(Low-Rank\r\nAdaptation of LLMs)，即LLMs的低秩适应，是参数高效微调最常用的方法。\r\nLoRA的本质就是用更少的训练参数来近似LLM全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。\r\n1.1实现流程\r\n\r\n\r\n在原始预训练语言模型旁边增加一个旁路，做降维再升维的操作来模拟内在秩；\r\n\r\n用随机高斯分布初始化\r\nA，用零矩阵初始化B，训练时固定预训练模型的参数，只训练矩阵 A 与矩阵\r\nB；\r\n训练完成后，将 B 矩阵与 A\r\n矩阵相乘后合并预训练模型参数作为微调后的模型参数。\r\n\r\n具体来讲，预训练权重矩阵 W0 ∈ ℝd × d\r\n，\r\n将增量参数矩阵 ΔW\r\n，表示为两个参数量更小的矩阵 B 和 A 的低秩近似,如下式\r\nW0 + ΔW = W0 + BA\r\n其中 B ∈ ℝd × r\r\n，A ∈ ℝr × d\r\n，秩r远小于d\r\n给定输入.x ∈ ℝd\r\n,添加LoRA后的输出.h ∈ ℝd\r\nh = (W0 + ΔW)x = W0x + BAx\r\nΔh = BAx\r\n1.2LoRA参数合并系数\r\n实际实现时以以下形式合并，其中α为超参数\r\n$\\mathbf{h}=(\\mathbf{W}_{0}+\\frac{\\alpha}{r}\\Delta\\mathbf{W})\\mathbf{x}$\r\n系数$\\frac{\\alpha}{r}$越大，LoRA微调权重的影响就越大，在下游任务上越容易过拟合\r\n系数$\\frac{\\alpha}{r}$越小，LoRA微调权重的影响就越小（微调的效果不明显，原始模型参数受到的影响也较少）\r\n一般来说，在给定任务上LoRA微调，让α为r的2倍数。\r\n1.3 LoRA的秩r如何选择\r\n目标：找到一个秩r，使BA无限接近ΔW的表达能力。\r\n秩r越大，拟合能力越强（甚至出现过拟合），但参与训练的参数量也随之增加。\r\n","categories":["-科研实习 -周记"]},{"title":"week2","url":"/2025/06/25/week2/","content":"diffusion model (李宏毅)笔记\r\n概念部分\r\n一般图像生成模型基本框架\r\ntext encoder → generation model → decoder\r\nFID(Frechet Inception\r\nDistance)\r\nFID是一种用于评估生成图像质量的度量标准\r\n\r\n特征提取 使用预训练的 Inception V3 模型（在 ImageNet\r\n数据集上训练的图像分类网络）作为特征提取器。输入图像（通常调整为 299×299\r\n的分辨率）会通过 Inception V3 前向传播，提取池化层（即 pool3\r\n层）的输出特征。这个特征是一个 2048 维的向量。\r\n特征分布假设 FID\r\n假设提取的特征向量服从多变量正态分布。对于真实图像集合X和生成图像集合G，分别计算特征的均值向量和协方差矩阵：\r\n真实图像特征均值 μr 协方差 Σr\r\n生成图像特征均值 μg 协方差 Σg\r\nFréchet 距离计算 Fréchet 距离用来衡量两个正态分布之间的差异FID = ∥μr − μg∥22 + Tr(Σr + Σg − 2(ΣrΣg)1/2)\r\n第一项衡量两个分布均值的欧几里得距离，表示分布中心的偏移，第二项衡量协方差矩阵的差异，反映分布形状和分散度的不同\r\n\r\n原理部分\r\nhttps://www.bilibili.com/video/BV14c411J7f2?spm_id_from=333.788.player.switch&amp;vd_source=257a40315247000b85510107fa6b747d&amp;p=4\r\n\r\n最大似然估计 https://zhuanlan.zhihu.com/p/55791843 \r\n扩散模型与能量模型，Score-Matching和SDE，ODE的关系 https://zhuanlan.zhihu.com/p/576779879\r\n\r\n疑问\r\n\r\n李宏毅认为噪声实际上不是一步一步加进x0的,而是一步实现的 \r\n但通过对一致性模型的学习，我了解到diffusion\r\nmodel的前向过程和逆向过程实际上都能表示为SDE过程，需要进行多次迭代，而consistency\r\nmodel就是为了解决这个问题，将SDE的随机项消除，转变为ODE过程，从而实现减少迭代次数，这是否与上图观点相悖？\r\n\r\n","categories":["-科研实习 -周记"]},{"title":"week5","url":"/2025/07/08/week5/","content":"\r\n"},{"title":"串口重定向","url":"/2025/07/08/%E4%B8%B2%E5%8F%A3%E9%87%8D%E5%AE%9A%E5%90%91/","content":"stm32串口重定向HAL库\r\n#include &lt;stdio.h&gt;// 包含标准输入输出头文件 int fputc(int ch,FILE *f)&#123;//采用轮询方式发送1字节数据，超时时间设置为无限等待HAL_UART_Transmit(&amp;huart1,(uint8_t *)&amp;ch,1,HAL_MAX_DELAY);return ch;&#125;int fgetc(FILE *f)&#123;uint8_t ch;// 采用轮询方式接收 1字节数据，超时时间设置为无限等待HAL_UART_Receive( &amp;huart1,(uint8_t*)&amp;ch,1, HAL_MAX_DELAY );return ch;&#125;\r\n","categories":["-单片机 -stm32"]},{"title":"使用循环缓冲区进行串口数据解析","url":"/2025/06/29/%E4%BD%BF%E7%94%A8%E5%BE%AA%E7%8E%AF%E7%BC%93%E5%86%B2%E5%8C%BA%E8%BF%9B%E8%A1%8C%E4%B8%B2%E5%8F%A3%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/","content":"stm32cubemx配置\r\n开启串口收发异步模式，开启串口空闲中断\r\n代码实现\r\n#include &quot;command.h&quot;// 指令的最小长度#define COMMAND_MIN_LENGTH 4// 循环缓冲区大小#define BUFFER_SIZE 128// 循环缓冲区uint8_t buffer[BUFFER_SIZE];// 循环缓冲区读索引uint8_t readIndex = 0;// 循环缓冲区写索引uint8_t writeIndex = 0;/*** @brief 增加读索引* @param length 要增加的长度*/void Command_AddReadIndex(uint8_t length) &#123;    readIndex += length;    readIndex %= BUFFER_SIZE;&#125;/*** @brief 读取第i位数据 超过缓存区长度自动循环* @param i 要读取的数据索引*/uint8_t Command_Read(uint8_t i) &#123;    uint8_t index = i % BUFFER_SIZE;    return buffer[index];&#125;/*** @brief 计算未处理的数据长度* @return 未处理的数据长度* @retval 0 缓冲区为空* @retval 1~BUFFER_SIZE-1 未处理的数据长度* @retval BUFFER_SIZE 缓冲区已满*///uint8_t Command_GetLength() &#123;//  // 读索引等于写索引时，缓冲区为空//  if (readIndex == writeIndex) &#123;//    return 0;//  &#125;//  // 如果缓冲区已满,返回BUFFER_SIZE//  if (writeIndex + 1 == readIndex || (writeIndex == BUFFER_SIZE - 1 &amp;&amp; readIndex == 0)) &#123;//    return BUFFER_SIZE;//  &#125;//  // 如果缓冲区未满,返回未处理的数据长度//  if (readIndex &lt;= writeIndex) &#123;//    return writeIndex - readIndex;//  &#125; else &#123;//    return BUFFER_SIZE - readIndex + writeIndex;//  &#125;//&#125;uint8_t Command_GetLength() &#123;    return (writeIndex + BUFFER_SIZE - readIndex) % BUFFER_SIZE;&#125;/*** @brief 计算缓冲区剩余空间* @return 剩余空间* @retval 0 缓冲区已满* @retval 1~BUFFER_SIZE-1 剩余空间* @retval BUFFER_SIZE 缓冲区为空*/uint8_t Command_GetRemain() &#123;    return BUFFER_SIZE - Command_GetLength();&#125;/*** @brief 向缓冲区写入数据* @param data 要写入的数据指针* @param length 要写入的数据长度* @return 写入的数据长度*/uint8_t Command_Write(uint8_t *data, uint8_t length) &#123;    // 如果缓冲区不足 则不写入数据 返回0    if (Command_GetRemain() &lt; length) &#123;        return 0;    &#125;    // 使用memcpy函数将数据写入缓冲区    if (writeIndex + length &lt; BUFFER_SIZE) &#123;        memcpy(buffer + writeIndex, data, length);        writeIndex += length;    &#125; else &#123;        uint8_t firstLength = BUFFER_SIZE - writeIndex;        memcpy(buffer + writeIndex, data, firstLength);        memcpy(buffer, data + firstLength, length - firstLength);        writeIndex = length - firstLength;    &#125;    return length;&#125;/*** @brief 尝试获取一条指令* @param command 指令存放指针* @return 获取的指令长度* @retval 0 没有获取到指令*/uint8_t Command_GetCommand(uint8_t *command) &#123;    // 寻找完整指令    while (1) &#123;        // 如果缓冲区长度小于COMMAND_MIN_LENGTH 则不可能有完整的指令        if (Command_GetLength() &lt; COMMAND_MIN_LENGTH) &#123;        return 0;        &#125;        // 如果不是包头 则跳过 重新开始寻找        if (Command_Read(readIndex) != 0xAA) &#123;        Command_AddReadIndex(1);        continue;        &#125;        // 如果缓冲区长度小于指令长度 则不可能有完整的指令        uint8_t length = Command_Read(readIndex + 1);        if (Command_GetLength() &lt; length) &#123;        return 0;        &#125;        // 如果校验和不正确 则跳过 重新开始寻找        uint8_t sum = 0;        for (uint8_t i = 0; i &lt; length - 1; i++) &#123;        sum += Command_Read(readIndex + i);        &#125;        if (sum != Command_Read(readIndex + length - 1)) &#123;        Command_AddReadIndex(1);        continue;        &#125;        // 如果找到完整指令 则将指令写入command 返回指令长度        for (uint8_t i = 0; i &lt; length; i++) &#123;        command[i] = Command_Read(readIndex + i);        &#125;        Command_AddReadIndex(length);        return length;    &#125;&#125;\r\n头文件： #ifndef INC_COMMAND_H_#define INC_COMMAND_H_#include &quot;main.h&quot;#include &lt;string.h&gt;uint8_t Command_Write(uint8_t *data, uint8_t length);uint8_t Command_GetCommand(uint8_t *command);#endif /* INC_COMMAND_H_ */\r\nmain.c: /* Private define ------------------------------------------------------------*//* USER CODE BEGIN PD */uint8_t readBuffer[10];/* USER CODE END PD */\r\n/* USER CODE BEGIN 0 */void HAL_UARTEx_RxEventCallback(UART_HandleTypeDef *huart, uint16_t Size)&#123;\tif (huart == &amp;huart2)&#123;\t\tCommand_Write(readBuffer, Size);\t\tHAL_UARTEx_ReceiveToIdle_IT(&amp;huart2, readBuffer, sizeof(readBuffer));\t&#125;&#125;/* USER CODE END 0 */\r\n/* USER CODE BEGIN 2 */HAL_UARTEx_ReceiveToIdle_IT(&amp;huart2, readBuffer, sizeof(readBuffer));uint8_t command[50];int commandLength = 0;/* USER CODE END 2 *//* Infinite loop *//* USER CODE BEGIN WHILE */while (1)&#123;    commandLength = Command_GetCommand(command);    if (commandLength != 0)&#123;        HAL_UART_Transmit(&amp;huart2, command, commandLength, HAL_MAX_DELAY);        for (int i = 2; i &lt; commandLength - 1; i += 2)&#123;            GPIO_PinState state = GPIO_PIN_SET;            if (command[i + 1] == 0x00)&#123;                state = GPIO_PIN_RESET;            &#125;            if (command[i] == 0x01)&#123;                HAL_GPIO_WritePin(RED_GPIO_Port, RED_Pin, state);            &#125;else if (command[i] == 0x02)&#123;                HAL_GPIO_WritePin(GREEN_GPIO_Port, GREEN_Pin, state);            &#125;else if (command[i] == 0x03)&#123;                HAL_GPIO_WritePin(BLUE_GPIO_Port, BLUE_Pin, state);            &#125;        &#125;    &#125;/* USER CODE END WHILE *//* USER CODE BEGIN 3 */&#125;\r\n","categories":["-单片机 -stm32"]},{"title":"注意力机制","url":"/2025/07/02/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","content":"注意力机制\r\n原论文：https://arxiv.org/pdf/1706.03762\r\n什么是注意力机制\r\n注意力机制就是让模型重点关注重要信息，忽略次要信息。注意力机制分为空间注意力和时间注意力，前者用于图像处理，后者用于自然语言处理.\r\n原理\r\nQuery：当前需要查询的目标，即当前输入的特征表示。\r\nKey：可以将每个单词的重要特征表示看作成 Key。\r\nValue：每个单词本身的特征向量看作为 Value，一般和\r\nKey成对出现，也就是我们常说的”键-值”对。\r\n\r\n核心公式（原论文中）：\r\n$Attention(Q,K,V)=Softmax(\\frac{QK^\\top}{\\sqrt{d_k}})V$\r\n步骤：\r\n\r\n先根据 Query，Key计算两者的相关性，然后再通过 softmax 函数得到\r\n注意力分数，使用 softmax 函数是为了使得所有的注意力分数在 [0,1]\r\n之间，并且和为1。\r\n相关性公式一般表示如下：\r\n$score(q,k_i)=softmax(\\alpha(q,k_i))=\\frac{exp(\\alpha(q,k_i))}{\\sum_1^jexp(\\alpha(q,k_j))}$\r\n其中α(q, ki)有很多变体：\r\ne.g. 在加性注意力中\r\nα(q, ki) = wvTtanh(Wqq + Wkk)\r\nW_q：Query对应的可训练矩阵\r\nW_k: Key对应的可训练矩阵\r\nw_v^T: Value对应的可训练矩阵\r\n(tanh为双曲正切函数，作为一种常见的激活函数)\r\ne.g.在缩放点积注意力中\r\n$\\alpha(q,k_i)=\\frac{QK^T}{\\sqrt{d}}$\r\n其中d为Keys的维度大小，除以sqrt{d}是为了使方差变小，训练梯度更新时更稳定\r\n将注意力分数加权求和，得到带注意力分数的Value\r\n\r\n自注意力机制（Self-Attention\r\nMechanism）\r\n\r\n2.1 Embedding 操作, 将向量x转化为a,a作为注意力机制的input data\r\n2.2 q, k 操作\r\nqi = Wqai\r\nki = Wkai\r\nvi = Wvai\r\n","categories":["-科研实习 -深度学习"]}]